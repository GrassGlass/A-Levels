\documentclass[../Notes.tex]{subfiles}
\usepackage{../Style/Diagrams}
\usepackage{../Style/Master}
\usepackage{../Style/boxes}
\usepackage{../Style/DefNoteFact}
\usepackage{../Style/QnsProof}
\usepackage{../Style/Thms}
\usepackage{../Style/Env}
\usepackage{../Style/NewCommands}
\begin{document}
\chapter{Linear Algebra}
\section{Vector spaces, subspaces, linear combinations, bases, linear transformations}
\begin{definition}{}{}
    A vector space (or linear space) \(V\) over a field \(\mathbb{F}\) consists of a set on which two operations (called addition and multiplication respectively here) are defined so that;
\begin{itemize}[label=(M1),leftmargin=*]
    \item[(A)](\(V\) is Closed Under Addition) For all \(\mathbf{x},\mathbf{y} \in V\), there exists a unique element \(\mathbf{x}+\mathbf{y} \in V\).
    \item[(M)](\(V\) is Closed Under Scalar Multiplication) For all elements \(a \in \mathbb{F}\) and elements \(\mathbf{x} \in V\), there exists a unique element \(a\mathbf{x} \in V\).
\end{itemize}
Such that the following properties hold:  
\begin{enumerate}[label=(VS {{\arabic*}}), leftmargin=*]
    \item \label{(VS 1)}(Commutativity of Addition) For all \(\mathbf{x},\mathbf{y} \in V\), we have \(\mathbf{x}+\mathbf{y}=\mathbf{y}+\mathbf{x}\).
    \item \label{(VS 2)}(Associativity of Addition) For all \(\mathbf{x},\mathbf{y},\mathbf{z} \in V\), we have \((\mathbf{x}+\mathbf{y})+\mathbf{z}=\mathbf{x}+(\mathbf{y}+\mathbf{z})\).
    \item \label{(VS 3)}(Existance of The Zero/Null Vector) There exists an element in \(V\) denoted by \({\mathbf{0}}\), such that \(\mathbf{x}+{\mathbf{0}}=\mathbf{x}\) for all \(\mathbf{x} \in V\).
    \item \label{(VS 4)}(Existance of Additive Inverses) For all elements \(\mathbf{x} \in V\), there exists an element \(\mathbf{y} \in V\) such that \(\mathbf{x}+\mathbf{y}={\mathbf{0}}\).
    \item \label{(VS 5)}(Multiplicative Identity) For all elements \(x \in V\), we have \(1\mathbf{x}=\mathbf{x}\), where 1 denotes the multiplicative identity in \(\mathbb{F}\).
    \item \label{(VS 6)}(Compatibility of Scalar Multiplication with Field Multiplication) For all elements \(a,b \in \mathbb{F}\) and elements \(\mathbf{x} \in V\), we have \((ab)\mathbf{x}=a(b\mathbf{x})\).
    \item \label{(VS 7)}(Distributivity of Scalar Multiplication over Vector Addition) For all elements \(a \in \mathbb{F}\) and elements \(\mathbf{x},\mathbf{y} \in V\), we have \(a(\mathbf{x}+\mathbf{y})=a\mathbf{x}+a\mathbf{y}\).
    \item \label{(VS 8)}(Distributivity of Scalar Multiplication over Field Addition) For all elements \(a,b \in \mathbb{F}\), and elements \(\mathbf{x} \in V\), we have \((a+b)\mathbf{x}=a\mathbf{x}+b\mathbf{x}\).
\end{enumerate}
\end{definition}
\begin{theorem}{}{}
    Let \(V\) be a vector space and \(W\) a subset of \(V\). Then \(W\) is a subspace of \(V\) iff the following 3 conditions hold for the operations defined in V.
            \begin{enumerate}[label=(\alph*)]
                \item \(\mathbf{0} \in W\) \label{Theorem 1.3(a)}
                \item \(\mathbf{x}+\mathbf{y} \in W\) whenever \(\mathbf{x} \in W\) and \(\mathbf{y} \in W\). \label{Theorem 1.3(b)}
                \item \(c\mathbf{x} \in W\) whenever \(c \in \mathbb{F}\) and \(\mathbf{x} \in W\). \label{Theorem 1.3(c)}
            \end{enumerate}
\end{theorem}
\begin{definition}{}{}
    A subset \(S\) of a vector space \(V\) \emph{generates} (or \emph{spans}) \(V\) iff \(\Span(S)=V\). In this case, we also say that the vectors of \(S\) generate (or span) \(V\).
\end{definition}
\begin{definition}{}{}
    Let \(V\) be a vector space and \(S\) a nonempty subset of \(V\). A vector \(v \in V\) is called a \emph{linear combination} of vectors of \(S\) iff there exists a finite number of vectors \(\mathbf{u}_1,\mathbf{u}_2,\dots,\mathbf{u}_n\) in \(S\) and scalars \(a_1,a_2,\dots,a_n\) in \(\mathbb{F}\) such that
            \[v=\sum_{i=1}^{n}{a_i\mathbf{u}_i}.\]
            In this case we also say that \(v\) is a linear combination of \(\mathbf{u}_1,\mathbf{u}_2,\dots,\mathbf{u}_n\) and call \(a_1,a_2,\dots,a_n\) the \emph{coefficients} of the linear combination
\end{definition}
\begin{definition}{}{}
    A set subset \(S\) of a vector space \(V\) is called \emph{linearly dependent} iff there exists a finite number of distinct vectors \(\mathbf{u}_1,\mathbf{u}_2,\dots,\mathbf{u}_n\)in \(S\) and scalars \(a_1,a_2,\dots,a_n\) not all zero, such that
            \[a_1\mathbf{u}_1+a_2\mathbf{u}_2+a_n\mathbf{u}_n=\mathbf{0}.\]
\end{definition}
\begin{definition}{}{}
    A \emph{basis} \(\beta\) for a vector space \(V\) is a linearly independent subset of \(V\) that generates \(V\). If \(\beta\) is a basis for \(V\), we also say that the vectors of \(\beta\) form a basis for \(V\).
\end{definition}
\begin{definition}{}{}
    Let \(V\) and \(W\) be vector spaces. We call a function \(T\colon V\to W\) a \emph{linear transformation from \(V\) to \(W\)} iff \(T(c\mathbf{x}+\mathbf{y})=cT(\mathbf{x})+T(\mathbf{y})\) for all \(\mathbf{x},\mathbf{y}\in V\) and \(c\in \mathbb{F}\).
\end{definition}
\begin{theorem}{The Rank-Nullity Theorem.}{}
    For any vector spaces \(V\) and \(W\), and a linear transformation \(T \colon V \to W\), it holds that
    \[\rank(T)+\nullity(T)=\dim(V).\]
\end{theorem}
\section{Matrices and systems of linear equations}
\begin{stbox}{General Information}
    \begin{itemize}
        \item Let \(\mathbf{A}\) be an \(m\times n\) matrix, and \(\mathbf{a}_j\) its \(j\)th column. For any \(\mathbf{x}=
        \begin{pmatrix}
            x_1 & x_2 & \cdots & x_n\\
        \end{pmatrix}^\top\), 
        \[\mathbf{A}\mathbf{x}=\sum_{j=1}^{n}{x_j}\mathbf{a}_j.\]
        \item Let \(\mathbf{A}\) and \(\mathbf{B}\) be matrices having \(n\) rows. For any matrix \(\mathbf{M}\) with \(n\) columns, we have
        \[\mathbf{M}
        \begin{pNiceArray}{c|c}
            \mathbf{A} & \mathbf{B}
        \end{pNiceArray}=
        \begin{pNiceArray}{c|c}
            \mathbf{M}\mathbf{A} & \mathbf{M}\mathbf{B}
        \end{pNiceArray}.\]
    \end{itemize}
\end{stbox}
\begin{definition}{}{}
    A system \(\mathbf{A}\mathbf{x}=\mathbf{b}\) is \emph{homogeneous} iff \(\mathbf{b}=0\); otherwise it is \emph{nonhomogeneous}.
\end{definition}
\begin{theorem}{}{}
    For any matrix, its row space, column space, and rank are identical.
\end{theorem}
\begin{theorem}{}{}
    A system \(\mathbf{A}\mathbf{x}=\mathbf{0}\) of \(m\) linear equations in \(n\) unknowns has a solution space of dimension \(n-\rank(A)\).
\end{theorem}
\begin{definition}{}{}
    A system \(\mathbf{A}\mathbf{x}=\mathbf{b}\) of linear equations is \emph{consistent} iff its solution set is nonempty; otherwise it is \emph{inconsistent}.
\end{definition}
\begin{theorem}{The Rouch√©-Capelli Theorem.}{}
    A system \(\mathbf{A}\mathbf{x}=\mathbf{b}\) is consistent iff \(\rank(\mathbf{A})=\rank(\mathbf{A}\vert \mathbf{b})\).
\end{theorem}
\begin{definition}{}{}
    A matrix is said to be in \emph{reduced row echelon form} iff
        \begin{itemize}
            \item Any row containing a nonzero entry precedes any row in which all the entries are zero (if any).
            \item The first nonzero entry in each row is the only nonzero entry in its column.
            \item The first nonzero entry in each row is 1 and it occurs in a column to the right of the first nonzero entry in the preceding row.
        \end{itemize}
\end{definition}
\begin{stbox}{General Information}
    \begin{itemize}
        \item Gaussian elimination. 
        \begin{itemize}
            \item In the forward pass, the augmented matrix is transformed into an upper triangular matrix in which the first nonzero entry of each row is 1 and it occurs in a column to the right of the first nonzero entry
            of each preceding row.
            \item  In the backward pass, the upper triangular matrix is transformed into reduced row echelon form by making the first nonzero entry of each row the only nonzero entry of its column.
        \end{itemize}
        \item Gaussian elimination always reduces a matrix to its rref form.
        \item Gaussian elimination always reduces \((\mathbf{A} \,\vert\, \mathbf{I}_n)\to (\mathbf{I}_n \,\vert\, \mathbf{A}^{-1})\).
        \item Let \(\mathbf{A}\coloneq
        \begin{pmatrix}
            \mathbf{a}_1&\mathbf{a}_2&\cdots&\mathbf{a}_n
        \end{pmatrix}\) be \(m\times n\) matrix, and \(\mathbf{A}'\coloneq
        \begin{pmatrix}
            \mathbf{a}'_1&\mathbf{a}'_2&\cdots&\mathbf{a}'_n
        \end{pmatrix}\) its rref. Then, \(\{\mathbf{a}_{k_1},\mathbf{a}_{k_2},\dots,\mathbf{a}_{k_m}\}\) is linearly independent iff \(\{\mathbf{a}'_{k_1},\mathbf{a}'_{k_2},\dots,\mathbf{a}'_{k_m}\}\) is. Moreover, the row space of \(\mathbf{A}\) and \(\mathbf{A}'\) are clearly identical.
        \item \href{https://math.stackexchange.com/a/802878}{(Example)} To find a basis for the intersection of the column spaces of \(\mathbf{A},\mathbf{B}\in\MS_{n \times n}(\mathbb{F})\), we reduce
        \[\begin{pmatrix}
            \mathbf{A}&\mathbf{B}
        \end{pmatrix}\to
        \begin{pmatrix}
            \mathbf{A'}&\mathbf{B'}
        \end{pmatrix}.\]
        Let \(\mathbf{c}_i\) and \(\mathbf{c'}_i\) be the \(i\)th columns of 
        \(\begin{pmatrix}
            \mathbf{A}&\mathbf{B}
        \end{pmatrix}\) and
        \(\begin{pmatrix}
            \mathbf{A'}&\mathbf{B'}
        \end{pmatrix}\), respectively.
        We compare the columns of \(\mathbf{A'}\) and \(\mathbf{B'}\) to find a basis \(\beta'\coloneq\{\mathbf{c}'_{i_1},\mathbf{c}'_{i_2},\dots,\mathbf{c}'_{i_r}\}\) for the intersection of the column spaces of \(\mathbf{A'}\) and \(\mathbf{B'}\). Then, \(\beta\coloneq\{\mathbf{c}_{i_1},\mathbf{c}_{i_2},\dots,\mathbf{c}_{i_r}\}\) is a basis for the intersection of the column spaces of \(\mathbf{A}\) and \(\mathbf{B}\).
        % \begin{enumerate}
        %     \item First notice that \(\mathbf{v} \in V\cap W\) iff
        %     \[\mathbf{v}=\mathbf{A}\mathbf{x}_1=\mathbf{B}\mathbf{x}_2\]
        %     for some \(\mathbf{x}_2\in \mathbb{F}^m\) and \(\mathbf{x}_2\in \mathbb{F}^k\). That is,
        %     \[\begin{pmatrix}
        %         \mathbf{A} & \mathbf{B} 
        %     \end{pmatrix}
        %     \begin{pmatrix}
        %         \mathbf{x_1}\\
        %         -\mathbf{x_2}
        %     \end{pmatrix}
        %     =\mathbf{0}.\]
        %     So, equivalently, we write
        %     \[\begin{pmatrix}
        %         \mathbf{A} & \mathbf{B} 
        %     \end{pmatrix}
        %     \mathbf{y}=\mathbf{0}.\]
        %     for some \(\mathbf{y}\in \mathbb{F}^{m+k}\). As such, by row reducing             
        %     \(\begin{pmatrix}
        %         \mathbf{A} & \mathbf{B} 
        %     \end{pmatrix}\), 
        %     we find a basis 
        %     \[\beta\coloneq\left\{
        %         \begin{pmatrix}
        %             \mathbf{u_1}\\
        %             \mathbf{u_1'}
        %         \end{pmatrix},
        %         \begin{pmatrix}
        %             \mathbf{u_2}\\
        %             \mathbf{u_2'}
        %         \end{pmatrix},\dots,
        %         \begin{pmatrix}
        %             \mathbf{u_r}\\
        %             \mathbf{u_r'}
        %         \end{pmatrix}
        %     \right\},\]
        %     where \(\mathbf{u}_i \in \mathbb{F}^m\) and \(\mathbf{u}_i \in \mathbb{F}^k\).
        %     Now, 
        %     % let \(\mathsf{u_i} \in \mathbb{F}^m\) be vector obtained by deleting all but the first \(m\) entries of \(\mathbf{u}_i\). Then,
        %     a generating set for \(V\cap W\) is
        %     \[\Gamma\coloneq\{\mathbf{A}\mathbf{u_1},\mathbf{A}\mathbf{u_2},\dots,\mathbf{A}\mathbf{u_r}\}.\]
        %     Alternatively, 
        %     % letting \(\mathsf{u'_i} \in \mathbb{F}^m\) be vector obtained by deleting all but the last \(k\) entries of \(\mathbf{u}_i\), 
        %     another generating set for \(V\cap W\) is
        %     \[\Delta\coloneq\left\{\mathbf{B}\mathbf{u'_1},\mathbf{B}\mathbf{u'_2},\dots,\mathbf{B}\mathbf{u'_r}\right\}.\]
        %     From here, it is simple to choose bases \(\gamma\subseteq\Gamma\) and \(\delta\subseteq\Delta\) for \(V\cap W\). 
            
        %     (Naturally, it holds that \(\mathbf{A}\mathbf{u_i}+\mathbf{B}\mathbf{u'_i}=0\).)
        %     \item An alternative method. By row reduction, we can calculate
        %     \begin{align*}
        %         r\coloneq\dim(V\cap W)&=\dim(U)+\dim(V)-\dim(U+V),\\
        %         &=\rank(\mathbf{A})+\rank(\mathbf{B})-\rank
        %         \begin{pmatrix}
        %             \mathbf{A}&\mathbf{B}
        %         \end{pmatrix},\\
        %         &=\rank\left(\mathbf{A}^\top\right)+\rank\left(\mathbf{B}^\top\right)-\rank
        %         \begin{pmatrix}
        %             \mathbf{A}^\top\\
        %             \mathbf{B}^\top
        %         \end{pmatrix}.
        %     \end{align*}
        %     Then, a basis for \(V\cap W\) can be formed by choosing \(r\) linearly independent vectors in \(V\cap W\). 
        %     % \(\begin{pmatrix}
        %     %     \mathbf{A}&\mathbf{B}
        %     % \end{pmatrix}\),
        %     %     or rows of \(\begin{pmatrix}
        %     %     \mathbf{A}^\top\\
        %     %     \mathbf{B}^\top
        %     % \end{pmatrix}\).
        %     \item \href{https://math.stackexchange.com/a/802878}{Another alternative}, probably the best option! Skip the row reduction of \(\mathbf{A}\) and \(\mathbf{B}\) in the above method. We just reduce
        % \[\begin{pmatrix}
        %     \mathbf{A}&\mathbf{B}
        % \end{pmatrix}\to
        % \begin{pmatrix}
        %     \mathbf{A'}&\mathbf{B'}
        % \end{pmatrix}.\]
        % Let \(\mathbf{c_i}\) and \(\mathbf{c'_i}\) be the \(i\)th column of 
        % \(\begin{pmatrix}
        %     \mathbf{A}&\mathbf{B}
        % \end{pmatrix}\) and
        % \(\begin{pmatrix}
        %     \mathbf{A'}&\mathbf{B'}
        % \end{pmatrix}\), respectively.
        % We compare the columns of \(A'\) and \(B'\) to find (with relative ease) a basis \(\beta'\coloneq\left\{\mathbf{c'_{i_1}},\mathbf{c'_{i_2}},\dots,\mathbf{c'_{i_r}}\right\}\) for the intersection of the column spaces of \(A'\) and \(B'\). Then, \(\beta\coloneq\{\mathbf{c_{i_1}},\mathbf{c_{i_2}},\dots,\mathbf{c_{i_r}}\}\) is a basis for \(V\cap W\) (the intersection of the column spaces of \(A\) and \(B\)).
        % \item \href{https://math.stackexchange.com/a/4837004}{A fourth method} for when I learn about orthogonal complements. 
        % \end{enumerate}
    \end{itemize}
\end{stbox}
\section{Determinants}
\begin{definition}{}{}
    Let \(\mathbf{A}\in \mathrm{M}_{n\times n}(\mathbb{F})\). If \(n=1\), so that \(A=(a_{11})\), we define \(\det(\mathbf{A})\coloneq a_{11}\). For \(n\geq 2\), we define \(\det(\mathbf{A})\) recursively as 
    \[\det(\mathbf{A})\coloneq \sum_{j=1}^{n}{(-1)^{1+j}}\mathbf{A}_{1j}\cdot \det(\widetilde{\mathbf{A}}_{1j}).\]
    The scalar \(\det(\mathbf{A})\) is called the \emph{determinant} of \(\mathbf{A}\) and is also denoted by \(\lvert \mathbf{A} \rvert\). The scalar 
    \[(-1)^{i+j}\det(\widetilde{\mathbf{A}}_{1j})\]
    is called the cofactor of the entry of \(\mathbf{A}\) in row \(i\), column \(j\).
\end{definition}
\begin{note}
    A matrix \(\mathbf{A}\) is invertible iff its determinant is nonzero. 
\end{note}
\begin{theorem}{}{}
    The determinant \(\det\colon\MS_{n \times n}(\mathbb{F})\to \mathbb{F}\) is an alternating \(n\)-linear function. 
    \begin{enumerate}[label=(\alph*)]
        \item Alternating: For \(\mathbf{A}\in\MS_{n \times n}(\mathbb{F})\) and any \(\mathbf{B}\) obtained from \(\mathbf{A}\) by interchanging any two rows of \(\mathbf{A}\),
        \[\det(\mathbf{B})=-\det(\mathbf{A}).\]
        \item \(n\)-linear: For any scalar \(k \in \mathbb{F}\) and vectors \(\mathbf{u},\mathbf{v},\mathbf{a}_i\in \mathbb{F}^n\),
        \[\det\begin{pmatrix}
            \mathbf{a}_1\\
            \mathbf{a}_2\\
            \vdots\\
            \mathbf{a}_{r-1}\\
            \mathbf{u}+k\mathbf{v}\\
            \mathbf{a}_{r+1}\\
            \vdots\\
            \mathbf{a}_n
        \end{pmatrix}=
        \det\begin{pmatrix}
            \mathbf{a}_1\\
            \mathbf{a}_2\\
            \vdots\\
            \mathbf{a}_{r-1}\\
            \mathbf{u}\\
            \mathbf{a}_{r+1}\\
            \vdots\\
            \mathbf{a}_n
        \end{pmatrix}+
        k\det\begin{pmatrix}
            \mathbf{a}_1\\
            \mathbf{a}_2\\
            \vdots\\
            \mathbf{a}_{r-1}\\
            \mathbf{v}\\
            \mathbf{a}_{r+1}\\
            \vdots\\
            \mathbf{a}_n
        \end{pmatrix}.\]
    \end{enumerate}
    In fact, \(\det\colon\MS_{n \times n}(\mathbb{F})\to \mathbb{F}\) is the \emph{unique} alternating \(n\)-linear function, such that \(\det(\mathbf{I})=1\).
\end{theorem}
\begin{corollary}{}{}
    Let \(\mathbf{A}\in\MS_{n \times n}(\mathbb{F})\). Then, for any matrix \(\mathbf{B}\) obtained by adding a scalar multiple of one row/column of \(\mathbf{A}\) to another, \(\det(\mathbf{B})=\det(\mathbf{A})\).
\end{corollary}
\begin{theorem}{}{}
    The determinant of a square matrix can be evaluated by cofactor expansion along any row. That is, if \(\mathbf{A} \in \mathrm{M}_{n\times n}(\mathbb{F})\), then for any integer \(1\leq i\leq n\),
        \[\det(\mathbf{A})=\sum_{j=1}^{n}{(-1)}^{i+j}\mathbf{A}_{ij}\cdot \det(\widetilde{\mathbf{A}}_{ij}).\] 
        Here, \(\widetilde{\mathbf{A}}_{ij}\) is the \((n-1)\times(n-1)\) matrix obtained from \(\mathbf{A}\) by deleting its \(i\)th row and \(j\)th column.
\end{theorem}
\begin{corollary}{}{}
    The determinant of any triangular matrix is the product of its diagonals.
\end{corollary}
\begin{theorem}{}{}
    Let \(A\) be an \(n\times n\) matrix. Then,
    \[\det(\mathbf{A})=\det(\mathbf{A}^\top).\]
    So, the determinant of a square matrix can also be evaluated by cofactor expansion along any column.
\end{theorem}
\begin{theorem}{}{}
    Let \(\mathbf{A}\) be an invertible \(n\times n\) matrix. Then,
    \[\mathbf{A}^{-1}=\frac{1}{\det(\mathbf{A})}\adj(A),\]
    where \(\adj(\mathbf{A})\) is the adjugate/classical adjoint of \(\mathbf{A}\). That is, the matrix whose \((i,j)\)th entry is the \((j,i)\)th cofactor \((-1)^{j+i}\det(\widetilde{\mathbf{A}}_{ji})\)
\end{theorem}
\begin{theorem}{}{}
    For any \(\mathbf{A},\mathbf{B} \in\MS_{n \times n}(\mathbb{F})\), we have \(\det(\mathbf{A}\mathbf{B})=\det(\mathbf{A})\cdot\det(\mathbf{B})\).
\end{theorem}
\section{Diagonalisation}
\begin{definition}{}{}
    A linear operator \(T\) on a finite-dimensional vector space \(V\) is called \emph{diagonalisable} iff there is an ordered basis \(\beta\) for \(V\) such that \([T]_\beta\) is a diagonal matrix. A square matrix \(\mathbf{A}\) is called diagonalisable iff \(L_\mathbf{A}\) is diagonalisable.
\end{definition}
\begin{definition}{}{}
    Let \(T\) be a linear operator on a vector space \(V\). A nonzero vector \(\mathbf{v}\in V\) is called an \emph{eigenvector} of \(T\) iff there exists a scalar \(\lambda\) such that \(T(\mathbf{v})=\lambda \mathbf{v}\). The scalar \(\lambda\) is called the \emph{eigenvalue} corresponding to the eigenvector \(\mathbf{v}\).

    Let \(\mathbf{A}\) be in \(\mathrm{M}_{n\times n}(\mathbb{F})\). A nonzero vector \(v\in \mathbb{F}^n\) is called an \emph{eigenvector} of \(\mathbf{A}\) iff \(v\) is an eigenvector of \(L_\mathbf{A}\); that is, iff \(\mathbf{A}v=\lambda v\) for some scalar \(\lambda\). The scalar \(\lambda\) is called the eigenvalue of \(\mathbf{A}\) corresponding to the eigenvector \(v\).
\end{definition}
\begin{definition}{}{}
    Let \(\mathbf{A}\in \mathrm{M}_{n\times n}(\mathbb{F})\). The polynomial \(f(t)=\det(\mathbf{A}-\lambda \mathbf{I}_n)\) is called the \emph{characteristic polynomial} of \(\mathbf{A}\).
\end{definition}
\begin{stbox}{}
    \begin{itemize}
        \item A matrix \(\mathbf{A}\in\mathrm{M}_{n\times n}(\mathbb{F})\) is diagonalizable iff there exists an ordered basis \(\{\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_n\}\) for \(\mathbb{F}^n\) consisting of eigenvectors of \(\mathbf{A}\), i.e. a eigenbasis. Furthermore, if \(\mathbf{Q}\) is the \(n\times n\) matrix whose \(j\)th column is \(\mathbf{v}_j\), then \(\mathbf{A}=\mathbf{Q}^{-1}\mathbf{D}\mathbf{Q}\) is a diagonal matrix such that \(d_{jj}\) is the eigenvalue of \(A\) corresponding to \(\mathbf{v}_j\). The matrix \(\mathbf{Q}\) is said to \emph{diagonalise} \(\mathbf{A}\).
        \item Hence, we obtain the following procedure to diagonalise a \(3\times 3\) matrix \(\mathbf{A}\) with three distinct eigenvalues.
        \begin{enumerate}
            \item Find the eigenvalues \(\lambda_1\), \(\lambda_2\), and \(\lambda_3\) of \(\mathbf{A}\) --- the roots of the characteristic polynomial of \(\mathbf{A}\). \hyperlink{characteristic-polynomial-roots}{This can be done using the GC}.
            \item Find an eigenvector \(\mathbf{v}_j\) corresponding to each eigenvalue \(\lambda_j\) by reducing \(\mathbf{A}-\lambda_j\mathbf{I}
            \).
            \item Let \(\mathbf{Q}=
            \begin{pmatrix}
                \mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3
            \end{pmatrix}\). Then,
            \[\mathbf{D}\coloneq \mathbf{Q}^{-1}\mathbf{A}\mathbf{Q}\]
            is a diagonal matrix.
        \end{enumerate} 
    \end{itemize}    
\end{stbox}
\begin{note}
    Let \(\mathbf{A}\) be a \(3\times 3\) real matrix, with the eigenvalue \(\lambda\). Then, the cross product of two linearly independent rows of \(\mathbf{A}-\lambda \mathbf{I}\) is an eigenvector of \(\mathbf{A}\).
\end{note}
\begin{theorem}{The Cayley-Hamiliton Theorem.}{}
    Let \(T\) be a linear operator on a finite dimensional vector space \(V\), and let \(f(t)\) be the characteristic polynomial of \(T\). Then \(f(T)=T_0\), the zero transformation. That is, \(T\) ``satisfies'' its characteristic equation.
\end{theorem}
\begin{corollary}{The Cayley-Hamiliton Theorem for Matrices.}{}
    Let \(\mathbf{A}\) be an \(n\times n\) matrix, and let \(f(t)\) be the characteristic polynomial of \(\mathbf{A}\). Then, \(f(\mathbf{A})=\mathbf{O}\), the \(n\times n\) zero matrix.  
\end{corollary}
\hypertarget{characteristic-polynomial-roots}{}
\begin{GCSkills}{}
    Finding eigenvalues of a matrix \(\mathbf{A}\) using the GC.
    \begin{enumerate}
        \item \texttt{2nd} \(\Longrightarrow\) \(\texttt{x}^{-1}\) (\texttt{matrix}) \(\Longrightarrow\) Key in the matrices \(\mathbf{A}\) and \(\mathbf{I}_3\) into \texttt{[A]} and \texttt{[I]}, respectively. 
        \item Plot \(\texttt{Y}_1=\det{(\texttt{[A]})}\).
        \item \texttt{2nd} \(\Longrightarrow\) \texttt{trace} \(\Longrightarrow\) \texttt{2:zero} \(\Longrightarrow\) Find the roots.
    \end{enumerate}
\end{GCSkills}
\section{Miscellaneous}
An asterisk denotes the conjugate transpose.
\begin{theorem}{}{}
    Let \(\mathbf{M}\in\MS_{n \times n}(\mathbb{K})\) be Hermitian (i.e. \(\mathbf{M}^{*}=\mathbf{M}\)), with eigenvectors \(\mathbf{u}\) and \(\mathbf{v}\) that correspond to the eigenvalues \(\lambda\) and \(\mu\). Then, \(\mathbf{u}\) and \(\mathbf{v}\) are orthogonal with respect to the standard inner product, if \(\lambda\neq\mu^{*}\). 
\end{theorem}
\begin{proof}
    Let \(\mathbf{u}\) and \(\mathbf{v}\) be eigenvectors of \(\mathbf{M}\) . Then, 
    \[\langle \mathbf{u},\mu\mathbf{v} \rangle=(\mathbf{M} \mathbf{v})^{*}\mathbf{u}=(\mathbf{v}^{*}\mathbf{M}^{*})\mathbf{u}=\mathbf{v}^{*}(\mathbf{M}^{*}\mathbf{u})=\mathbf{v}^{*}(\lambda \mathbf{u})=\langle \lambda \mathbf{u},\mathbf{v} \rangle.\]
    As such, \((\lambda-\mu^{*})\langle \mathbf{u},\mathbf{v} \rangle=0\). Hence, \(\langle \mathbf{u},\mathbf{v} \rangle=0\).
\end{proof}
\begin{example}{}{}
    Consider a computer that rounds each calculated value to \(n\) decimal places, which is then used in later calculations as if it were exact. Perform, for \(n=3\) and \(n=4\), this procedure to find the solution \(\mathbf{x}=
    \begin{pmatrix}
        x_1 & x_2 & x_3
    \end{pmatrix}^\top\) to \(\mathbf{A}\mathbf{x}=\mathbf{b}\). Then, find \(\sum_{i=1}^{3}{\delta_i^2}\) where \(\delta_i\) is the difference between the exact value of \(x_i\) and the one found by the computer: this gives a measure for the accuracy of the calculated values. Comment on the difference in results.

    \rule{20cm-137.0549pt}{0.05mm}

    \vspace{0.5\baselineskip} One extra decimal place of accuracy in the working (a factor of 10) had led to a significant increase in the measure of accuracy (by a factor of around 250). 
\end{example}
\newpage
\section{Conics}
Given a conic section defined by \(Ax^2+Bxy+Cy^2+Dx+Ey+F=0\), we may wish to find its lines of symmetry, center, radii, etc. The core idea is simple: complete the square, to express \(Ax^2+Bxy+Cy^2\) in the form \(a(x')^2+b(y')^2\), for some linear combinations \(x'\) and \(y'\) of \(x\) and \(y\). Then, the initial equation becomes \(a(x')^2+b(y')^2+d(x')+e(y')+F=0\), which is easily reduced to the standard form for conics. Before that, we need to develop some machinery.
\begin{definition}{}{}
    Let \(\mathbb{F}\) be a field not of characteristic two. A function \(K\colon \mathbb{F}^n\to \mathbb{F}\) is called a \emph{quadratic form on \(\mathbb{F}^n\)} if there exists a symmetric matrix \(A\in\MS_{n \times n}(\mathbb{F})\), such that \(K(\mathbf{x})=\mathbf{x}^{\top} \mathbf{A}\mathbf{x}\) for all \(\mathbf{x}\in \mathbb{F}^n\).
\end{definition}
\begin{note}
    Let \(\mathbb{F}\) be a field not of characteristic two and scalars \(a_i\in \mathbb{F}\). The polynomial \(f\colon \mathbb{F}^n\to \mathbb{F}\) given by \(f(t_1,t_2,\dots,t_n)=\sum_{i\leq j}{a_{ij}t_it_j}\) is a quadratic form. In fact, the matrix \(\mathbf{A}\) with 
    \[\mathbf{A}_{ij}=
    \begin{cases}
        a_{ii} &\text{if }i=j\\
        a_{ij}/2 &\text{if }i\neq j
    \end{cases}\]
    gives us our desired quadratic form \(K(\mathbf{x})=\mathbf{x}^{\top}\mathbf{A}\mathbf{x}\).
\end{note}
\begin{theorem}{}{}
    \hypertarget{thm:quadratic-forms-real}{}
    Let \(K(\mathbf{x})=\mathbf{x}^{\top} \mathbf{A}\mathbf{x}\) be a quadratic form on a finite-dimensional real inner product space \(V\). There exists an orthonormal eigenbasis \(\beta\coloneq\{\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_n\}\) for \(\mathbf{A}\) and eigenvalues \(\lambda_i\), such that \(K\left( \sum_{i=1}^{n}{t_i \mathbf{v}_i} \right)=\sum_{i=1}^{n}{\lambda_it_i^2}\) for all \(t_i\in \mathbb{R}\).
\end{theorem}
We now return to our initial problem on conics. We first diagonalise the matrix 
\[\begin{pmatrix}
    A & B/2\\
    B/2 & C
\end{pmatrix}\]
to find its eigenvalues \(\lambda\) and \(\mu\), then the corresponding unit eigenvectors \(\mathbf{u}=
\begin{pmatrix}
    \alpha & \beta
\end{pmatrix}^\top\) and \(\mathbf{v}=
\begin{pmatrix}
    \gamma & \delta
\end{pmatrix}^\top\). Then,
\[\begin{pmatrix}
    x\\
    y
\end{pmatrix}=
\begin{pmatrix}
    \mathbf{u} & \mathbf{v}
\end{pmatrix}
\begin{pmatrix}
    t_1\\
    t_2
\end{pmatrix}=
\begin{pmatrix}
    \alpha t_1+\gamma t_2\\
    \beta t_1+\delta t_2
\end{pmatrix}.\]
Furthermore, \(Ax^2+Bxy+Cy^2=\lambda t_1^2+\mu t_2^2\) by \hyperlink{thm:quadratic-forms-real}{1.28}. Therefore, 
\begin{align*}
    \lambda t_1^2+\mu t_2^2+D(\alpha t_1+\gamma t_2)+E(\beta t_1+\delta t_2)+F&=0\\
    \lambda\left( t_1+\frac{D\alpha+E\beta}{2\lambda} \right)^2+\mu\left( t_2+\frac{D\gamma+E\delta}{2\mu} \right)^2-\frac{(D\alpha+E\beta)^2}{4\lambda}-\frac{(D\gamma+E\delta)^2}{4\mu}+F&=0 \qquad\text{(if \(\lambda,\mu\neq 0\))}
\end{align*}
gives an equivalent form for our conic section. 

Since our basis \(\{\mathbf{u},\mathbf{v}\}\) is orthonormal, the above change of basis from the standard ordered basis to \(\{\mathbf{u},\mathbf{v}\}\) is an isometry (that maps \(\mathbf{0}\) to itself). It is obtained via a rotation and/or reflection. Hence, the radii are \(\sqrt{\lambda/k}\) and \(\sqrt{\mu/k}\), for \(4k=(D\alpha+E\beta)^2/\lambda+(D\gamma+E\delta)^2/\mu-F\); the center is \(\left(-\frac{D\alpha+E\beta}{2\lambda},-\frac{D\gamma+E\delta}{2\mu} \right)\). Notice that \(t_1=0\) and \(t_2=0\) are parallel to the axes of symmetry. i.e. \(\mathbf{u}\) and \(\mathbf{v}\) are parallel to the axes of symmetry of our conic. As such, the lines of symmetry are 
\[y+\frac{D\gamma+E\delta}{2\mu}=\frac{\beta}{\alpha}\left( x+\frac{D\alpha+E\beta}{2\lambda} \right) \qquad\text{and}\qquad y+\frac{D\gamma+E\delta}{2\mu}=\frac{\delta}{\gamma}\left( x+\frac{D\alpha+E\beta}{2\lambda} \right).\]
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{../Diagrams/rotated-conics.jpg}
    \caption{Rotated and translated conic.}
    \label{fig:rotated-conic}
\end{figure}
\begin{note}
    Without orthonormality, i.e. an isometric change of coordinates, even if we were successful in reducing our conic to the form \(a(x')^2+b(y')^2=f\), it may not prove to be a useful form. Consider the ellipse \(2x^2+2xy+y^2=1\). Clearly, \(x^2+(x+y)^2=1\). But, this gives a circle in \((x,x+y)\) coordinates; we can't deduce much about our initial conic. So, little meaning is found in such a factorisation. 
\end{note}
% Let \(\mathbf{x}\) and \(\mathbf{y}\) be the eigenvectors of \(\mathbf{M}\) corresponding to \(\lambda\) and \(\mu\). Since \(\mathbf{M}\) is symmetric, i.e. \(\mathbf{M}^T=\mathbf{M}\),
% \[\mathbf{x}\cdot\mu\mathbf{y}=(\mathbf{M}\mathbf{y})^{T}\mathbf{x}=(\mathbf{y}^T \mathbf{M}^T)\mathbf{x}=\mathbf{y}^T (\mathbf{M} \mathbf{x})=\mu \mathbf{x}\cdot \mathbf{y}.\]
% As such, \((\lambda-\mu)\mathbf{x}\cdot \mathbf{y}=0\). Hence, \(\mathbf{x}\cdot \mathbf{y}=0\).
% Consider the equation \(x^2+y=0\) and the matrix
% \(\mathbf{M}=\begin{pmatrix}
%     1 & 0\\
%     1 & 1
% \end{pmatrix}\). Then, \(x'=x\) and \(y'=x+y\). However, even though \(1^2-1=0\), we have \(1^2+(1-1)=1\neq 0\).
    
\chapter{Numerical Methods}
\begin{stbox}{General Information}
    \begin{itemize}
        \item The parity of the degree of a real polynomial is the same as that of its number of real roots.
        \item Let the real polynomial \(p\) given by \(p(x)=a_{2n}x^{2n}+a_{2n-1}x^{2n-1}+\dots+a_0\) have coefficients \(a_n>0\) and \(a_0<0\). Then, it has at least one positive and one negative root.
        \item To show that there a continuous function \(f\) attains a root in an interval \([a,b]\), we find two values \(x<y\) in the interval (e.g. \(a<b\)) such that \(f(a)f(b)<0\). i.e. show that \(f\) changes sign in \([a,b]\). Then, \emph{by continuity}, a root of \(f\) must lie in \([a,b]\).
        \item To further show that the root is \emph{unique} in \([a,b]\), it suffices to prove that \(f\) is \emph{strictly} monotone on \([a,b]\).
        \item Suppose we have some function \(f \colon \mathbb{R}\to \mathbb{R}\) with a root \(\alpha\), whose value we want to approximate. There are three ways to obtain this approximation.
        \begin{enumerate}
            \item Linear interpolation on an interval \([a,b]\) containing \(\alpha\). Our approximation is
            \[\frac{a \lvert f(b) \rvert+b \lvert f(a) \rvert}{\lvert f(a) \rvert+\lvert f(b) \rvert}.\]
            \begin{itemize}
                \item The sequence \(\{x_n\}\) of approximations \emph{always} converges to \(\alpha\).
                \item The smaller \(\lvert f''(x) \rvert\) is (i.e. the slower the gradient \(f'(x)\) changes) near \(\alpha\), the faster the rate of convergence.
                \item Error:
                \begin{table}[H]
                    \centering
                    \begin{tabular}{|Sc|Sc|Sc|}
                        \hline
                        Concave/Gradient & Positive & Negative\\
                        \hline
                        Upwards \(\bigcup\) & \textcolor{blue}{underestimation} & \textcolor{red}{overestimation}\\
                        \hline
                        Downwards \(\bigcap\) & \textcolor{red}{overestimation} & \textcolor{blue}{underestimation}\\
                        \hline
                    \end{tabular}
                    \caption{Approximation errors when using linear interpolation.}
                    \label{table:linear-interpolaion}
                \end{table}
                \item See Figure \ref{fig:linear-interpolation} for an illustration.
                \footnotetext{Screw trying to make nice diagonal cells. Pain. Suffering.}
            \end{itemize}
        \end{enumerate}
    \end{itemize}
\end{stbox}
\begin{note}
    At every iteration of linear interpolation, we must ensure that \(\alpha\in [a,x_n]\). Otherwise \(x_n\) may not approximate \(\alpha\). If \(\alpha\notin [a,x_n]\), simply consider \(\alpha\in [x_n,b]\) (or any other suitable interval) instead.
\end{note}
\begin{note}
    It is important to show which interval we are interpolating on, not just the iteratively obtained values. We can present our working using the table below.
    \begin{table}[H]
        \centering
        \begin{tabular}{|Sc|Sc|Sc|Sc|Sc|}
            \hline
            \(a\) & \(f(a)\) & \(b\) & \(f(b)\) & \(\dfrac{a \lvert f(b) \rvert+b \lvert f(a) \rvert}{\lvert f(a) \rvert+\lvert f(b) \rvert}\)\\
            \hline
            \(a\) & \(f(a)>0\) & \(b\) & \(f(b)<0\) & \(x_1\)\\
            \hline
            \(x_1\) & \(f(x_1)>0\) & \(b\) & \(f(b)<0\) & \(x_2\)\\
            \hline
            \(x_1\) & \(f(x_1)>0\) & \(x_2\) & \(f(x_2)<0\) & \(x_3\)\\
            \hline
            \(\vdots\) & \(\vdots\) & \(\vdots\) & \(\vdots\) & \(\vdots\)\\
            \hline
        \end{tabular}
        \caption{Required working for linear interpolation.}
        \label{table:linear-interpolation-presentation}
    \end{table}
\end{note}
\begin{stbox}{}
    \begin{itemize}
        \item[]
        \begin{enumerate}
            \item[2.] Fixed-point Iteration. First select a function \(F \colon \mathbb{R}\to \mathbb{R}\), such that \(F(\alpha)=\alpha\), and choose some initial approximation \(x_0\) to \(\alpha\). Then, we recursively define \(x_{n+1} \coloneq F(x_n)\). We want \(x_n\to \alpha\).
            \begin{itemize}
                \item Convergence behavior
                \begin{table}[H]
                    \centering
                    \begin{tabular}{|Sc|Sc|Sc|Sc|}
                        \hline
                        Behvaior of \(\lvert F'(x) \rvert\) & Converges? & Rate of convergence\\
                        \hline
                        \(\lvert F'(x) \rvert<1\) and is small near \(\alpha\) & \textcolor{green!70!black}{\checkmark} & \textcolor{green!70!black}{fast}\\
                        \hline
                        \(\lvert F'(x) \rvert<1\) but is close to 1 near \(\alpha\) & \textcolor{green!70!black}{\checkmark} & \textcolor{blue}{slow}\\
                        \hline
                        \(\lvert F'(x) \rvert\geq 1\) near \(\alpha\) & \textcolor{red}{\(\times\)} & -\\
                        \hline
                    \end{tabular}
                    \caption{Convergence behavior of fixed-point iterations.}
                    \label{table:fixed-point-iteration}
                \end{table}
                \item See Figure \ref{fig:fixed-point-iteration} for an illustration.
            \end{itemize}
        \end{enumerate}
    \end{itemize}
\end{stbox}
\begin{note}
    We must write out \emph{all} iterations, not just the final two. The working below is sufficient.

    \rule{20cm-137.0549pt}{0.05mm}

    \vspace{0.5\baselineskip} Let \(x_0=\rule{0.5cm}{0.05mm}\) and \(x_{n+1}=F(x_n)\), \(x\geq 0\).
    \begin{align*}
        x_1&=\rule{0.5cm}{0.05mm}\\
        x_2&=\rule{0.5cm}{0.05mm}\\
        &\vdotswithin{=}\\
        x_{m-1}&=\rule{0.5cm}{0.05mm}\\
        x_m&=\rule{0.5cm}{0.05mm}
    \end{align*}
    Therefore, \(\alpha=x_m\) (\(k\) d.p.), since \(f(x_m-0.0\cdots05)f(x_m+0.0\cdots05)=\rule{0.5cm}{0.01mm}<0\).
\end{note}
\begin{stbox}{}
    \begin{itemize}
        \item[]
        \begin{enumerate}
            \item[3.] The Newton-Raphson Method. Let \(\alpha\) be a root of the function \(f\colon \mathbb{R}\to \mathbb{R}\). The Newton-Raphson formula is
            \[x_{n+1}\coloneq x_n-\frac{f(x_n)}{f'(x_n)}.\]
            \begin{itemize}
                \item The Newton-Raphson method fails in the following cases.
                \begin{enumerate}
                    \item The gradient at \(x_0\) is too gentle.
                    \item The gradient changes too rapidly.
                    \item The initial approximation \(x_0\) is too far from the root \(\alpha\).
                    \item There is a turning point between the initial approximation \(x_0\) and the root \(\alpha\).
                    \item There is a point of inflection --- where the concavity changes/the sign of \(f''(x)\) changes.
                \end{enumerate}
                \item Error: 
                \begin{table}[H]
                    \centering
                    \begin{tabular}{|Sc|Sc|Sc|}
                        \hline
                        Concave/Gradient & Positive & Negative\\
                        \hline
                        Upwards \(\bigcup\) & \textcolor{red}{overestimation} & \textcolor{blue}{underestimation}\\
                        \hline
                        Downwards \(\bigcap\) & \textcolor{blue}{underestimation} & \textcolor{red}{overestimation}\\
                        \hline
                    \end{tabular}
                    \caption{Approximation errors when using the Newton-Raphson method.}
                    \label{table:newton-raphson}
                \end{table}
                \item See Figure \ref{fig:newton's-method} for an illustration.
            \end{itemize}
        \end{enumerate}
    \end{itemize}
\end{stbox}
\begin{note}
    We must write out \emph{all} iterations, not just the final two. One way to present our working is as follows.

    \rule{20cm-137.0549pt}{0.05mm}

    \vspace{0.5\baselineskip} Let \(x_0=\rule{0.5cm}{0.05mm}\) and \(x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}=\rule{0.5cm}{0.05mm}\)\,, \(x\geq 0\).
    \begin{align*}
        x_1&=\rule{0.5cm}{0.05mm}\\
        x_2&=\rule{0.5cm}{0.05mm}\\
        &\vdotswithin{=}\\
        x_{m-1}&=\rule{0.5cm}{0.05mm}\\
        x_m&=\rule{0.5cm}{0.05mm}
    \end{align*}
    Therefore, \(\alpha=x_m\) (\(k\) d.p.), since \(f(x_m-0.0\cdots05)f(x_m+0.0\cdots05)=\rule{0.5cm}{0.01mm}<0\).
\end{note}
\begin{note}
    Explain whether \(x_0=\rule{0.5cm}{0.05mm}\) is a suitable starting value for using the Newton-Raphson method to find an approximation to \(\alpha\).
    
    \rule{20cm-137.0549pt-25pt}{0.05mm}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{../Diagrams/newton's-method-prelim.pdf}
        \caption{\href{https://www.desmos.com/calculator/mq2elll5k3}{(Desmos)}}
        \label{fig:newton's-method-prelims}
    \end{figure}
    \begin{enumerate}
        \item Since \(x_0=\rule{0.5cm}{0.05mm}\) is very close to the stationary point, the tangent to the curve \(y=f(x)\) has a very gentle gradient. Thus, it cuts the \(x\)-axis far away from the initial approximation. 
        \item Furthermore, as \(x_0=\rule{0.5cm}{0.05mm}\) is to the left/right of the minimum/maximum point \(x=\rule{0.5cm}{0.05mm}\), the values of the gradient \(f'(x_n)\) will be negative/positive for all \(n\geq 0\). Hence, \(x_n\) converges to the root \(\beta\) instead \(\alpha\). 
    \end{enumerate}
    (The second point may be omitted if it is irrelevant.)
\end{note}
\begin{note}
    Suppose a question asks for the approximation of a root to \(k\) significant figures/\(k\) decimal places. Then: 
    \begin{enumerate}
        \item We leave our iterative approximations \(x_n\) to at least \(k+2\) significant figures/\(k+2\) decimal places.
        \item We continue the iterative process till two consecutive ones agree up to \(k\) significant figures/\(k\) decimal places.
    \end{enumerate}
\end{note}
\begin{note}
    Perform \rule{1.5cm}{0.01mm} (e.g. linear interpolation) to obtain an approximation for \(\alpha\), correct to two decimal places. Justify whether this approximation is sufficiently accurate.

    \rule{20cm-137.0549pt}{0.05mm}
    
    \vspace{0.5\baselineskip} Suppose our approximation is some \(a=1.00\), then we note the sign of \(f\) at \(a\pm 0.\highlight[yellow]{00}5\). (For an arbitrary number of s.f. or d.p., simply adjust the value \(0.\highlight[yellow]{00}5\) accordingly. E.g. for 3 d.p. we instead use \(0.\highlight[yellow]{000}5\)). Our working should look similar to the following:
    \begin{center}
        \parbox{0.9\textwidth}{
            Since \(f(0.995)=\rule{0.5cm}{0.01mm}<0\) and \(f(1.005)=\rule{0.5cm}{0.01mm}>0\), we conclude that \(1.00\) is a sufficiently accurate approximation, at 2 d.p..
        }
    \end{center}
\end{note}
\begin{note}
    The \emph{error obtained} when using an approximation should be the \emph{absolute} difference of the true value and the approximation. 
\end{note}
\begin{note}
    Use the results of part (i) and the differential equation \(dy/dx=\sin(xy)\) to estimate the \(x\)-coordinate \(x_P\) of \(P\). 

    \rule{20cm-137.0549pt}{0.05mm}

    \vspace{0.5\baselineskip} The maximum point occurs when \(dy/dx=\sin(xy)=0\), i.e. \(xy=k\pi\) where \(k\in \mathbb{Z}\). From (i), \(\frac{dy}{dx}\big|_{x=5/3}\approx 0.643>0\) so \(y_P>y(5/3)\approx 2.0468\). Hence, \(x_P\approx \pi/2.04679402=1.53\) (3.s.f).
\end{note}
\begin{GCSkills}{}
    Linear interpolation: finding an approximation to a root in \([a,b]\) up to \(n\) decimal places.
    \begin{enumerate}
        \item \(Y_1=f(x)\),
        \item \(a \to A\) and \(b \to B\),
        \item \(\dfrac{B \lvert Y_1(A) \rvert+A \lvert Y_1(B) \rvert}{\lvert Y_1(A) \rvert+\lvert Y_1(B) \rvert}\),
        \item \(\text{Ans}\to A \text{ or } B\) (choose the one that has the opposite sign to Ans),
        \item Repeat steps 4 to 5,
        \item Terminate this process when the approximations are consistent up to \(n\) decimal places.
    \end{enumerate}
    \end{GCSkills}
    You can freely enter any function and shift the initial values in the Desmos graphs below!
    \newpage
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.69\textwidth]{../Diagrams/linear-interpolation.pdf}
        \caption{An illustration of linear interpolation \href{https://www.desmos.com/calculator/yz71wfvkrl}{(Desmos)}.}
        \label{fig:linear-interpolation}
        % The second and third lines are wrong! 
    \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.69\textwidth]{../Diagrams/fixed-point-iteration/fixed-point-iteration-desmos.pdf}
        \caption{An illustration of fixed-point iteration \href{https://www.desmos.com/calculator/t9mnqtmhxw}{(Desmos)}.}
        \label{fig:fixed-point-iteration}
    \end{figure}
    % \begin{figure}[H]
    %     \centering
    %     \includegraphics[width=\textwidth]{../Diagrams/fixed-point-iteration/fixed-point-iteration.pdf}
    %     \caption{An illustration of fixed-point iteration.}
    %     \label{fig:fixed-point-iteration}
    % \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.69\textwidth]{../Diagrams/newton's-method.pdf}
        \caption{An illustration of Newton's Method \href{https://www.desmos.com/calculator/izkg4ynlfp}{(Desmos)}.}
        \label{fig:newton's-method}
    \end{figure}
\end{document}