\documentclass[../Notes.tex]{subfiles}
\usepackage{../Style/Diagrams}
\usepackage{../Style/Master}
\usepackage{../Style/boxes}
\usepackage{../Style/DefNoteFact}
\usepackage{../Style/QnsProof}
\usepackage{../Style/Thms}
\usepackage{../Style/Env}
\usepackage{../Style/NewCommands}
\begin{document}
\chapter{Linear Algebra}
\begin{definition}{}{}
    A vector space (or linear space) \(V\) over a field \(\mathbb{F}\) consists of a set on which two operations (called addition and multiplication respectively here) are defined so that;
\begin{itemize}[label=(M1),leftmargin=*]
    \item[(A)](\(V\) is Closed Under Addition) For all \(\mathbf{x},\mathbf{y} \in V\), there exists a unique element \(\mathbf{x}+\mathbf{y} \in V\).
    \item[(M)](\(V\) is Closed Under Scalar Multiplication) For all elements \(a \in \mathbb{F}\) and elements \(\mathbf{x} \in V\), there exists a unique element \(a\mathbf{x} \in V\).
\end{itemize}
Such that the following properties hold:  
\begin{enumerate}[label=(VS {{\arabic*}}), leftmargin=*]
    \item \label{(VS 1)}(Commutativity of Addition) For all \(\mathbf{x},\mathbf{y} \in V\), we have \(\mathbf{x}+\mathbf{y}=\mathbf{y}+\mathbf{x}\).
    \item \label{(VS 2)}(Associativity of Addition) For all \(\mathbf{x},\mathbf{y},\mathbf{z} \in V\), we have \((\mathbf{x}+\mathbf{y})+\mathbf{z}=\mathbf{x}+(\mathbf{y}+\mathbf{z})\).
    \item \label{(VS 3)}(Existance of The Zero/Null Vector) There exists an element in \(V\) denoted by \({\mathbf{0}}\), such that \(\mathbf{x}+{\mathbf{0}}=\mathbf{x}\) for all \(\mathbf{x} \in V\).
    \item \label{(VS 4)}(Existance of Additive Inverses) For all elements \(\mathbf{x} \in V\), there exists an element \(\mathbf{y} \in V\) such that \(\mathbf{x}+\mathbf{y}={\mathbf{0}}\).
    \item \label{(VS 5)}(Multiplicative Identity) For all elements \(x \in V\), we have \(1\mathbf{x}=\mathbf{x}\), where 1 denotes the multiplicative identity in \(\mathbb{F}\).
    \item \label{(VS 6)}(Compatibility of Scalar Multiplication with Field Multiplication) For all elements \(a,b \in \mathbb{F}\) and elements \(\mathbf{x} \in V\), we have \((ab)\mathbf{x}=a(b\mathbf{x})\).
    \item \label{(VS 7)}(Distributivity of Scalar Multiplication over Vector Addition) For all elements \(a \in \mathbb{F}\) and elements \(\mathbf{x},\mathbf{y} \in V\), we have \(a(\mathbf{x}+\mathbf{y})=a\mathbf{x}+a\mathbf{y}\).
    \item \label{(VS 8)}(Distributivity of Scalar Multiplication over Field Addition) For all elements \(a,b \in \mathbb{F}\), and elements \(\mathbf{x} \in V\), we have \((a+b)\mathbf{x}=a\mathbf{x}+b\mathbf{x}\).
\end{enumerate}
\end{definition}
    \begin{stbox}{General Information}
        \begin{itemize}
            \item  Let \(V\) be a vector space and \(W\) a subset of \(V\). Then \(W\) is a subspace of \(V\) iff the following 3 conditions hold for the operations defined in V.
            \begin{enumerate}[label=(\alph*)]
                \item \(\mathbf{0} \in W\) \label{Theorem 1.3(a)}
                \item \(\mathbf{x}+\mathbf{y} \in W\) whenever \(\mathbf{x} \in W\) and \(\mathbf{y} \in W\). \label{Theorem 1.3(b)}
                \item \(c\mathbf{x} \in W\) whenever \(c \in \mathbb{F}\) and \(\mathbf{x} \in W\). \label{Theorem 1.3(c)}
            \end{enumerate}
            \item A subset \(S\) of a vector space \(V\) \emph{generates} (or \emph{spans}) \(V\) iff \(\Span(S)=V\). In this case, we also say that the vectors of \(S\) generate (or span) \(V\).
            \item Let \(V\) be a vector space and \(S\) a nonempty subset of \(V\). A vector \(v \in V\) is called a \emph{linear combination} of vectors of \(S\) iff there exists a finite number of vectors \(u_1,u_2,\dots,u_n\) in \(S\) and scalars \(a_1,a_2,\dots,a_n\) in \(\mathbb{F}\) such that
            \[v=\sum_{i=1}^{n}{a_iu_i}.\]
            In this case we also say that \(v\) is a linear combination of \(u_1,u_2,\dots,u_n\) and call \(a_1,a_2,\dots,a_n\) the \emph{coefficients} of the linear combination
            \item A set subset \(S\) of a vector space \(V\) is called \emph{linearly dependent} iff there exists a finite number of distinct vectors \(u_1,u_2,\dots,u_n\)in \(S\) and scalars \(a_1,a_2,\dots,a_n\) not all zero, such that
            \[a_1u_1+a_2u_2+a_nu_n=\mathbf{0}.\]
            \item A \emph{basis} \(\beta\) for a vector space \(V\) is a linearly independent subset of \(V\) that generates \(V\). If \(\beta\) is a basis for \(V\), we also say that the vectors of \(\beta\) form a basis for \(V\).
            \item The Rank-Nullity Theorem: For any vector spaces \(V\) and \(W\), and a linear operator \(T \colon V \to W\), it holds that
            \[\rank(T)+\nullity(T)=\dim(V).\]
            \item For any matrix, its row space, column space, and rank are identical.
            \item A system \(\mathbf{A}\mathbf{x}=\mathbf{b}\) is \emph{homogeneous} iff \(\mathbf{b}=0\); otherwise it is \emph{nonhomogeneous}.
            \item A system \(\mathbf{A}\mathbf{x}=\mathbf{b}\) of \(m\) linear equations in \(n\) unknowns has a solution space of dimension \(n-\rank(A)\).
            \item A system \(\mathbf{A}\mathbf{x}=\mathbf{b}\) of linear equations is \emph{consistent} iff its solution set is nonempty; otherwise it is \emph{inconsistent}.
            \item The Rouch√©-Capelli theorem: A system \(\mathbf{A}\mathbf{x}=\mathbf{b}\) is consistent iff \(\rank(\mathbf{A})=\rank(\mathbf{A}\vert \mathbf{b})\).
            \item A matrix is said to be in \emph{reduced row echelon form} iff
            \begin{itemize}
                \item Any row containing a nonzero entry precedes any row in which all the entries are zero (if any).
                \item The first nonzero entry in each row is the only nonzero entry in its column.
                \item The first nonzero entry in each row is 1 and it occurs in a column to the right of the first nonzero entry in the preceding row.
            \end{itemize}
            \item Gaussian elimination. 
            \begin{itemize}
                \item In the forward pass, the augmented matrix is transformed into an upper triangular matrix in which the first nonzero entry of each row is 1 and it occurs in a column to the right of the first nonzero entry
                of each preceding row.
                \item  In the backward pass, the upper triangular matrix is transformed into reduced row echelon form by making the first nonzero entry of each row the only nonzero entry of its column.
            \end{itemize}
            \item Let \(\mathbf{A}\) be an \(m\times n\) matrix, and \(\mathbf{a}_j\) its \(j\)th column. For any \(\mathbf{x}=
            \begin{pmatrix}
                x_1 & x_2 & \cdots & x_n\\
            \end{pmatrix}^\top\), 
            \[\mathbf{A}\mathbf{x}=\sum_{j=1}^{n}{x_j}\mathbf{a}_j.\]
            \item Let \(\mathbf{A}\) and \(\mathbf{B}\) be matrices having \(n\) rows. For any matrix \(\mathbf{M}\) with \(n\) columns, we have
            \[\begin{pNiceArray}{c|c}
                \mathbf{A} & \mathbf{B}
            \end{pNiceArray}=
            \begin{pNiceArray}{c|c}
                \mathbf{M}\mathbf{A} & \mathbf{M}\mathbf{B}
            \end{pNiceArray}.\]
            \item Finding a basis for an intersection of subspaces. Let \(V\) and \(W\) be subspaces of \(\mathbb{F}^n\) generated by the columns of the \(n\times m\) matrix \(\mathbf{A}\) and \(n\times k\) matrix \(\mathbf{B}\), respectively. To find a basis for the subspace \(V\cap W\), first notice that \(\mathbf{v} \in V\cap W\) iff
            \[\mathbf{v}=\mathbf{A}\mathbf{x}_1=\mathbf{B}\mathbf{x}_2\]
            for some \(\mathbf{x}_2\in \mathbb{F}^m\) and \(\mathbf{x}_2\in \mathbb{F}^k\). That is,
            \[\begin{pmatrix}
                \mathbf{A} & \mathbf{B} 
            \end{pmatrix}
            \begin{pmatrix}
                \mathbf{x_1}\\
                -\mathbf{x_2}
            \end{pmatrix}
            =\mathbf{0}.\]
            So, equivalently, we write
            \[\begin{pmatrix}
                \mathbf{A} & \mathbf{B} 
            \end{pmatrix}
            \mathbf{y}=\mathbf{0}.\]
            for some \(\mathbf{y}\in \mathbb{F}^{m+k}\). As such, by row reducing             
            \(\begin{pmatrix}
                \mathbf{A} & \mathbf{B} 
            \end{pmatrix}\), 
            we find a basis 
            \[\beta\coloneq\left\{
                \begin{pmatrix}
                    \mathbf{u_1}\\
                    \mathbf{u_1'}
                \end{pmatrix},
                \begin{pmatrix}
                    \mathbf{u_2}\\
                    \mathbf{u_2'}
                \end{pmatrix},\dots,
                \begin{pmatrix}
                    \mathbf{u_r}\\
                    \mathbf{u_r'}
                \end{pmatrix}
            \right\},\]
            where \(\mathbf{u}_i \in \mathbb{F}^m\) and \(\mathbf{u}_i \in \mathbb{F}^k\).
            Now, 
            % let \(\mathsf{u_i} \in \mathbb{F}^m\) be vector obtained by deleting all but the first \(m\) entries of \(\mathbf{u}_i\). Then,
            a generating set for \(V\cap W\) is
            \[\Gamma\coloneq\{\mathbf{A}\mathbf{u_1},\mathbf{A}\mathbf{u_2},\dots,\mathbf{A}\mathbf{u_r}\}.\]
            Alternatively, 
            % letting \(\mathsf{u'_i} \in \mathbb{F}^m\) be vector obtained by deleting all but the last \(k\) entries of \(\mathbf{u}_i\), 
            another generating set for \(V\cap W\) is
            \[\Delta\coloneq\left\{\mathbf{B}\mathbf{u'_1},\mathbf{B}\mathbf{u'_2},\dots,\mathbf{B}\mathbf{u'_r}\right\}.\]
            From here, it is simple to choose bases \(\gamma\subseteq\Gamma\) and \(\delta\subseteq\Delta\) for \(V\cap W\). 
            
            (Naturally, it holds that \(\mathbf{A}\mathbf{u_i}+\mathbf{B}\mathbf{u'_i}=0\).)
            \item \href{https://math.stackexchange.com/a/802878}{An alternative method}. We again reduce the matrix 
            \(\begin{pmatrix}
                \mathbf{A}&\mathbf{B}
            \end{pmatrix}\).
            Then, we simply compare the columns of \(A\) and \(B\).
            \item \href{https://math.stackexchange.com/a/4837004}{A third method} for when I learn about orthogonal complements. 
            \item The determinant of a square matrix can be evaluated by cofactor expansion along any row. That is, if \(\mathbf{A} \in \mathrm{M}_{n\times n}(\mathbb{F})\), then for any integer \(1\leq i\leq n\),
            \[\det(\mathbf{A})=\sum_{j=1}^{n}{(-1)}^{i+j}\mathbf{A}_{ij}\cdot \det(\widetilde{\mathbf{A}}_{ij}).\] 
            Here, \(\widetilde{\mathbf{A}}_{ij}\) is the \((n-1)\times(n-1)\) matrix obtained from \(\mathbf{A}\) by deleting its \(i\)th row and \(j\)th column.
            \item The determinant of a square matrix can also be evaluated by cofactor expansion along any column, since
            \[\det(\mathbf{A})=\det(\mathbf{A}^\top).\]
            \item A matrix \(\mathbf{A}\) is invertible iff its determinant is nonzero. 
            \item Let \(\mathbf{A}\) be an invertible \(n\times n\) matrix. Then, for some elementary row matrices \(\mathbf{E}_1\) to \(\mathbf{E}_p\),
            \[\mathbf{E}_p\mathbf{E}_{p-1}\dots \mathbf{E}_1(\mathbf{A} \,\vert\, \mathbf{I}_n)=\mathbf{A}^{-1}(\mathbf{A} \,\vert\, \mathbf{I}_n)=(\mathbf{I}_n \,\vert\, \mathbf{A}^{-1}).\]
            In other words, we can perform Gaussian elimination, so that \((\mathbf{A} \,\vert\, \mathbf{I}_n)\to (\mathbf{I}_n \,\vert\, \mathbf{A}^{-1})\).
            \item Alternatively,
            \[\mathbf{A}^{-1}=\frac{1}{\det(\mathbf{A})}\adj(A),\]
            where \(\adj(\mathbf{A})\) is the adjugate / classical adjoint of \(\mathbf{A}\). That is, the matrix whose \((i,j)\)th entry is the \((j,i)\)th cofactor \((-1)^{j+i}\det(\widetilde{\mathbf{A}}_{ji})\)
        \end{itemize}
    \end{stbox}
    
    \chapter{Numerical Methods}
    \begin{stbox}{General Information}
        \begin{itemize}
            \item The parity of the degree of a real polynomial is the same as that of its number of real roots.
            \item Let the real polynomial \(p\) given by \(p(x)=a_{2n}x^{2n}+a_{2n-1}x^{2n-1}+\dots+a_0\) have coefficients \(a_n>0\) and \(a_0<0\). Then, it has at least one positive and one negative root.
            \item Suppose we have some function \(f \colon \mathbb{R}\to \mathbb{R}\) with a root \(\alpha\), whose value we want to approximate. There are three ways to obtain this approximation.
            \begin{enumerate}
                \item Linear interpolation on an interval \([a,b]\) containing \(\alpha\). We let \(x_0\coloneq b\) and
                \[x_{i+1}\coloneq\frac{a \lvert f(x_i) \rvert+x_i \lvert f(a) \rvert}{\lvert f(a) \rvert+\lvert f(x_i) \rvert}.\]
                \begin{itemize}
                    \item \emph{Additional notes.}
                \end{itemize}
                \item Fixed-point Iteration. First select a function \(F \colon \mathbb{R}\to \mathbb{R}\), such that \(F(\alpha)=\alpha\), and choose some initial approximation \(x_0\) to \(\alpha\). Then, we recursively define \(x_{n+1} \coloneq F(x_n)\). The desired convergence behavior is for \(x_n\) to approach \(\alpha\).
                \begin{itemize}
                    \item \emph{Additional notes.}
                \end{itemize}
            \end{enumerate}
        \end{itemize}
    \end{stbox}
    \begin{GCSkills}{}
        Linear interpolation: finding an approximation to a root in \([a,b]\) up to \(n\) decimal places.
        \begin{enumerate}
          \item \(Y_1=f(x)\),
          \item \(a \to A\) and \(b \to B\),
          \item \(\dfrac{B \lvert Y_1(A) \rvert+A \lvert Y_1(B) \rvert}{\lvert Y_1(A) \rvert+\lvert Y_1(B) \rvert}\),
          \item \(\text{Ans}\to A \text{ or } B\) (choose the one that has the opposite sign to Ans),
          \item Repeat steps 4 to 5,
          \item Terminate this process when the approximations are consistent up to \(n\) decimal places.
        \end{enumerate}
      \end{GCSkills}
\end{document}