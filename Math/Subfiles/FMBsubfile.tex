\documentclass[../Notes.tex]{subfiles}
\usepackage{../Style/Diagrams}
\usepackage{../Style/Master}
\usepackage{../Style/boxes}
\usepackage{../Style/DefNoteFact}
\usepackage{../Style/QnsProof}
\usepackage{../Style/Thms}
\usepackage{../Style/Env}
\usepackage{../Style/NewCommands}
\begin{document}
\chapter{Continuous Random Variables}
\begin{stbox}{General Information}
  \begin{itemize}
    \item A function \(f \colon \mathbb{R}\to \mathbb{R}\) is a \emph{probability mass function} (pdf) of a continuous random variable \(X\) iff \(f\) is nonnegative and \(\int_{-\infty}^{\infty}f(x)\,dx=1\).
    \item For any probability mass function \(f\), we have \(\Prob(a\leq X\leq b)=\int_{a}^{b}f(x)\,dx\). Whether the inequality is strict or nonstrict does not affect the above identity. 
    \item A \emph{mode} of \(X\) is any value \(m\) such that \(f(m)\) is maximum.
    \item A \emph{cumulative distribution function} (cdf) \(F \colon \mathbb{R}\to [0,1]\) of a random variable \(X\) is defined by
    \[F(x)\coloneq P(X\leq x)=\int_{-\infty}^{x}f(x)\,dx.\]
    \item When writing out the cdf as a piecewise function, we explicitly write out the range of values for each case. We reserve the use of ``otherwise'' for pdf's.
    \item Any cdf is continuous and nondecreasing.
    \item Let \(X\) be a continuous random variable with cdf \(F\). To find the pdf \(g\) of any \(y(X)\), we first find its cdf, then differentiate. We achieve this by reverse engineering \(y(X)\leq y\) to find an inequality that relates \(X\) with \(y\). E.g. \(e^X\leq y\) iff \(X\leq \ln(y)\).
    \item A \emph{median} of \(X\) is any value \(m\) such that \(\Prob(X\leq m)=F(m)=1/2\).
    \item Mean/Expectation: 
    \[\mu=\E(X)\coloneq \int_{-\infty}^{\infty}xf(x)\,dx \qquad\text{and}\qquad \E(g(X))=\int_{-\infty}^{\infty}g(x)f(x)\,dx.\]
    \item Important property: 
    \[\E(ag(X)\pm bh(x))=a\E(g(X))\pm\E(h(X)).\]
    \item Variance: 
    \[\Var(X)\coloneq \E(X^2)-[\E(X)]^2.\]
    \item Important property:
    \[\Var(aX\pm b)=a^2\Var(X).\]
  \end{itemize}
\end{stbox}

\chapter{Special Continuous Random Variables}
\begin{definition}{}{}
  A continuous random variable \(X\) has a \emph{normal distribution} with mean \(\mu\) and standard deviation \(\sigma\), denoted by \(X \sim \operatorname{N}(\mu,\sigma^2)\), iff its pdf \(f\) is such that 
  \[f(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right).\]
\end{definition}
\begin{stbox}{General Information}
  \begin{itemize}
    \item A normal distribution is symmetrical about the line \(x=\mu\). That is 
    \[\Prob(X\leq\mu-\delta)=\Prob(X\geq\mu+\delta)\]
    for each \(\delta>0\). Note that the mean, median, and mode coincide with \(\mu\).
    \item Properties of the normal distribution. Let \(X\) and \(Y\) be independent, such that \(X \sim \operatorname{N}(\mu,\sigma^2)\) and \(Y \sim \operatorname{N}(m,s^2)\). Then, for any \(n \in \mathbb{N}\) and \(x\), \(y \in \mathbb{R}\),  
    \begin{itemize}
      \item \(nX \sim \operatorname{N}(n\mu,n^2\sigma^2)\),
      \item \(X_1+X_2+\cdots+X_n \sim \operatorname{N}(n\mu,n\sigma^2)\),
      \item \(aX\pm bY \sim \operatorname{N}(a\mu\pm bm,a^2\sigma^2+b^2s^2)\).
    \end{itemize}
    \item At times, the question may be phrased in a misleading manner. Try using some inference to figure out the intended interpretation.
  \end{itemize}
\end{stbox}
\begin{example}{}{}
  ``The mass of the padding is \(30\%\) of the mass of a randomly selected light bulb of mass \(L\). Find the probability that a light bulb with padding has mass \(c\).'' 
    
  Then for any light bulb of mass \(L_1\), the mass of the padding is \(0.3L_2\) (and \emph{not} \(0.3L_1\)). i.e. we are to find \(\Prob(L_1+0.3L_2)\).
\end{example}
\begin{stbox}{}
  \begin{itemize}
    \setcounter{enumi}{3}
    \item A variable \(Z\sim \operatorname{N}(0,1)\) is said to follow the \emph{standard} normal distribution.

    \emph{Note}: \(Z\) is reserved for this purpose.
    \item Let \(X \in \operatorname{N}(\mu,\sigma^2)\). Then, \(\frac{X-\mu}{\sigma}\) follows the standard normal distribution. 
    \item What \texttt{Tail} do we select for \texttt{invNorm}?
    \begin{center}
      \begin{tabular}{|Sc|Sc|}
        \hline
        \(\Prob(X<x)=p\) & \texttt{LEFT}\\
        \hline
        \(\Prob(-x<X<x)=p\) & \texttt{CENTER}\\
        \hline
        \(\Prob(X>x)=p\) & \texttt{RIGHT}\\
        \hline
      \end{tabular}
    \end{center}
    \item When using \texttt{invNorm} on an inequality, what should the sign be? For simplicity, we write \(\mathscr{L}(p)=\texttt{invNorm}(p,0,1,\texttt{RIGHT})\), and \(\mathscr{R}(p)=\texttt{invNorm}(p,0,1,\texttt{LEFT})\). Then,
    \begin{center}
      \begin{tabular}{|Sc|Sc|Sc|}
        \hline
        \(\Prob(Z>z)\geq p\) & \(z\leq \mathscr{L}(p)\)\\
        \hline
        \(\Prob(Z>z)\leq p\) & \(z\geq \mathscr{L}(p)\)\\
        \hline
        \(\Prob(Z<z)\geq p\) & \(z\geq \mathscr{R}(p)\)\\
        \hline
        \(\Prob(Z<z)\leq p\) & \(z\leq \mathscr{R}(p)\)\\
        \hline
      \end{tabular}
    \end{center}
  \end{itemize}
\end{stbox}
\begin{example}{}{}
  Suppose we want to find the least integer value of \(m\) for which \(\Prob(Z>1-m)\geq 1/2\).

  Then, using \texttt{invNorm (RIGHT)}, we infer that \(z\leq 0\), \emph{not} \(z\geq 0\). An illustration: 
  \begin{center}
    \includegraphics[width=\textwidth]{../images/Special-Continuous-Random-Variables-Example-Illustration.jpg}
  \end{center}
\end{example}
\begin{definition}{}{}
  A continuous random variable \(X\) has a \emph{uniform distribution} over the interval \((a,b)\), which is denoted by \(X \sim \operatorname{U}(a,b)\), iff its pdf \(f\) is such that
    \[f(x)=\begin{cases}
      \frac{1}{b-a} &\text{if \(a<x<b\),}\\
      0 &\text{otherwise.}
    \end{cases}\] 
\end{definition}
\begin{definition}{}{}
  A continuous random variable \(Y\) has an (negative) exponential distribution, which we denote with \(Y\sim \operatorname{Exp}(\lambda)\), iff its pdf \(g\) is such that
    \[g(Y)=
    \begin{cases}
      \lambda e^{-\lambda x} &\text{if \(x\geq 0\)},\\
      0 &\text{otherwise.}
    \end{cases}\]
  (An exponential distribution models time between occurrences.)
\end{definition}
\begin{note}
  Let \(Y \sim \operatorname{Exp}(\lambda)\), then
  \[\Prob(Y>z+y \,\vert\, Y>y)=\Prob(Y>z) \qquad\text{and}\qquad\Prob(Y<z+y \,\vert\, Y>y)=\Prob(Y<z).\]
  % In general, it also holds that 
  % \[\Prob(V>z+y \,\vert\, V>y)=1-\Prob(V<z+y \,\vert\, V>y)\]
  % for any random variable \(V\).
\end{note}
\begin{stbox}{}
  \begin{itemize}
    \item Expectation and variance:
    \begin{center}
      \begin{tabular}{|Sc|Sc|Sc|}
        \hline
        Distribution & Expectation & Variance\\
        \hline
        \(X\sim \operatorname{U}(a,b)\) & \(\dfrac{a+b}{2}\) & \(\dfrac{(b-a)^2}{12}\)\\
        \hline
        \(Y\sim \operatorname{Exp}(\lambda)\) & \(\dfrac{1}{\lambda}\) & \(\dfrac{1}{\lambda^2}\)\\
        \hline
      \end{tabular} 
    \end{center}
    \emph{Note}: We need to remember the expectation and variance for the uniform distribution, as it is not provided in the MF26 formula sheet (unlike all other distributions).
    \item \emph{Warning}: The G.C. tends to incorrectly process an integral if its upper and lower bounds contain \(\pm \text{E}99\).
    \item Let \(T\) be the time taken between two consecutive arrivals and \(\#\sim\operatorname{Po}(\lambda t)\) the number of arrivals in time \(t\). Then, 
    \[\Prob(T>t)=\Prob(\#=0)=e^{-4t}.\]
    As such, the probability that there is at least one arrival in an interval of time \(t\) is 
    \[\Prob(T\leq t)=1-e^{-4t}.\]  
  \end{itemize}
\end{stbox}
\chapter{Sampling and Estimation}
\begin{definition}{}{}
  A sample is a finite subset of the population.
\end{definition}
\begin{definition}{}{}
  A random sample is a sample selected such that each member of the population has an equal probability of being selected into the sample.
\end{definition}
\begin{note}
  State, in context, what it means for the sample to be random.
  \begin{center}
    \parbox{0.9\textwidth}{
      It means that \hly{every [a member of the population]} has \hly{an equal probability} of being \hly{selected into the sample}. 
    }
  \end{center}
\end{note}
\begin{note}
  Explain why the sample would actually not be random.
  \begin{center}
    \parbox{0.9\textwidth}{
      [Contextual reason], so \hly{not all} the [members of the population] have an \hly{equal probability of being selected into the sample}. 
    }
  \end{center}
\end{note}
\begin{definition}{}{}
  Any statistic \(T\) derived from a random sample and used to estimated an unknown population parameter \(\theta\) is known as an \emph{estimator}. It is an \emph{unbiased} estimator iff \(\E(T)=\theta\). If \(T\) is unbiased we commonly write \(\hat{\theta}\) for \(T\).
\end{definition}
\begin{stbox}{General Information}
  \begin{itemize}
    \item Either write \(\hat{\mu}\highlight[yellow]{=\bar{x}}=\dots\) or write out ``Unbiased estimate of the population mean \(\mu\), \(\widebar{x}=\dots\)'' Same holds for other population parameters \(\theta\).
    \item Estimators you should know:
    \item 
    % *A table where Parameter is under another column
    % \begin{center}
    %   \begin{tabular}{|Sc|Sc|Sc|Sc|Sc|}
    %     \hline
    %     \multicolumn{2}{|Sc|}{Parameter} & Estimator & Unbiased? & Formula\\
    %     \hline
    %     Population Mean & \(\mu\) & \(\widebar{X}\) & \checkmark & \(\dfrac{X_1+X_2+\dots+X_n}{n}\)\\
    %     \hline
    %     \multirow{2}{*}[-1.2cm]{Population Variance} & \multirow{2}{*}[-1.2cm]{\(\sigma^2\)} & \(\sigma_n^2\) & \(\times\) & 
    %     \begin{minipage}{3cm}
    %       \begin{center}
    %         \(\dfrac{\sum{(X_i-\widebar{X})^2}}{n}\)\\[1mm]
    %         \(\dfrac{\sum{X_i^2}}{n}-\widebar{X}^2\)
    %       \end{center}
    %     \end{minipage}\\
    %     \cline{3-4}
    %     & & \(S^2\) & \checkmark & 
    %     \begin{minipage}{5cm}
    %       \begin{center}
    %       \(\dfrac{n}{n-1}\sigma_n^2\)\\[1mm]
    %       \(\dfrac{\sum(X_i-\widebar{X})^2}{n-1}\)\\[1mm]
    %       \(\dfrac{1}{n-1}\left[ \sum X_i^2-\dfrac{(\sum X_i)^2}{n} \right]\)
    %       \end{center}
    %     \end{minipage}\\
    %     \hline
    %     Population Proportion & \(p\) & \(P_s\) & \checkmark & \(\dfrac{X}{n}\)\\
    %     \hline
    %   \end{tabular}
    % \end{center}
    \begin{center}
      \resizebox{0.94\textwidth}{!}{\begin{tabular}{|Sc|Sc|Sc|Sc|}
        \hline
        Parameter & Estimator & Unbiased? & Formula\\
        \hline
        Population Mean \(\mu\) & Sample Mean \(\widebar{X}\) & \checkmark & \(\dfrac{X_1+X_2+\dots+X_n}{n}\)\\
        \hline
        \multirow{2}{*}[-1.2cm]{Population Variance \(\sigma^2\)} & Sample Variance \(\sigma_n^2\) & \(\times\) & 
        \begin{minipage}{3cm}
          \begin{center}
            \(\dfrac{\sum{(X_i-\widebar{X})^2}}{n}\)\\[1mm]
            \(\dfrac{\sum{X_i^2}}{n}-\widebar{X}^2\)
          \end{center}
        \end{minipage}\\
        \cline{2-4}
        & \(S^2\) & \checkmark & 
        \begin{minipage}{5cm}
          \begin{center}
          \(\dfrac{n}{n-1}\sigma_n^2\)\\[1mm]
          \(\dfrac{\sum(X_i-\widebar{X})^2}{n-1}\)\\[1mm]
          \(\dfrac{1}{n-1}\left[ \sum X_i^2-\dfrac{(\sum X_i)^2}{n} \right]\)
          \end{center}
        \end{minipage}\\
        \hline
        Population Proportion \(p\) & Sample Proportion \(P_s\) & \checkmark & \(\dfrac{X}{n}\)\\
        \hline
      \end{tabular}}
    \end{center}
    \item Let \(X\) be a random variable following \emph{any distribution}, and suppose we have a random sample \(X_1,X_2,\dots,X_n\) of size \(n\geq 50\). Then by CLT (Central Limit Theorem), since \(n\geq 50\) is large, 
    \[\widebar{X}\sim \Normal\left(\mu,\frac{\sigma^2}{n}\right) \qquad\text{and}\qquad X_1+X_2+\dots+X_n\sim \Normal(n\mu,n\sigma^2)\]
    \emph{approximately}.
    \item Assumptions when using CLT:
    \begin{itemize}
      \item The sample is random.
      \item Each \(X_i\) is independent and identically distributed.
    \end{itemize}
    \item Suppose \(X\sim \Normal(\mu,\sigma^2)\) is known and we pick a \emph{particular} sample. Then,
    \begin{center}
      \begin{tabular}{|Sc|Sc|Sc|}
        \hline
        Distribution & Is An Approximation?\\
        \hline
        \(\widebar{X} \sim \Normal(\mu,\sigma^2)\) & No\\
        \hline
        \(\widebar{X} \sim \Normal(\widebar{x},\sigma^2)\) & Yes\\
        \hline
        \(\widebar{X} \sim \Normal(\mu,s^2)\) & Yes\\
        \hline
        \(\widebar{X} \sim \Normal(\widebar{x},s^2)\) & Yes\\
        \hline
      \end{tabular}
    \end{center}
    % ADD NAMES of the parameters and estimators to previous table
    So, if we obtain any of the latter three in solving a question, we must write ``\(X\sim \Normal(\rule{3mm}{0.1mm},\rule{3mm}{0.1mm})\)approximately'' (even though we knew \(X\) \emph{exactly} follows a normal distribution!)
    \item Pooled estimators. First assume we have two populations, from which we select a random sample of size \(n_1\) and \(n_2\). We let \(\widebar{X}_1\) and \(S_1^2\) denote the sample mean and unbiased estimator for variance, respectively, for the first sample. Similarly define \(\widebar{X}_2\) and \(S_2^2\), for the second sample.
    \begin{center}
      \begin{tabular}{|Sc|Sc|}
        \hline
        Parameter & Unbiased Pooled Estimator\\
        \hline
         Mean  & \(\hat{\mu}=\dfrac{n_1\widebar{X}_1+n_2\widebar{X}_2}{n_1+n_2}\)\\
         \hline
         Variance & \(S_p^2=\dfrac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}\)\\
         \hline
      \end{tabular}
    \end{center}
  \end{itemize}
\end{stbox}
The following definition is found in 
\href{https://www.amazon.com/Introduction-Mathematical-Statistics-8th-Whats-dp-0134686993/dp/0134686993/ref=dp_ob_title_bk}{Hogg-McKean-Craig}. Similar definitions are also found in 
\href{https://www.amazon.sg/Mathematical-Statistics-Applications-William-Mendenhall/dp/0495110817#customerReviews}{Wackerly-Mendenhall-Schaefer}
and \href{https://www.amazon.com/Probability-Statistical-Inference-Statistics-Monographs/dp/0824703790}{Nitis Mukhopadhyay}.
\begin{definition}{}{}
  Let \(X_1,X_2,\dots,X_n\) be a sample on a random variable \(X\), where \(X\) has pdf \(f(x;\theta)\), \(\theta \in \Omega\). Let \(0<\alpha<1\) be specified. Let \(L=L(X_1,X_2,\dots,X_n)\) and \(U=U((X_1,X_2,\dots,X_n))\) be two statistics. We say that the interval \((L,U)\) is a \((1-\alpha)100\%\) \emph{confidence interval} for \(\theta\) iff 
  \[1-\alpha=P_\theta[\theta \in (L,U)].\]
  That is, the probability that the interval contains \(\theta\) is \(1-\alpha\), which is called the \emph{confidence coefficient} or \emph{confidence level} of the interval.
\end{definition}
\begin{stbox}{}
  \begin{itemize}
    \item We cannot write ``a \(1-\alpha\) (e.g. 0.95) confidence interval''. The \(1-\alpha\) must always be expressed as a \emph{percentage}.
    % \item Let \(0<\alpha<1\) and \(X_1,X_2,\dots,X_n\) be a sample on a random variable \(X\). Suppose \(n\) is large (\(n\geq 50\)). Then, CLT allows us to obtain the approximation
    % \[\frac{\widebar{X}-\mu}{\frac{S}{\sqrt{n}}}=Z\sim \Normal(0,1).\]
    % Rewriting \(\Prob(-z_{1-\alpha/2}<Z<z_{1-\alpha/2})=1-\alpha\) gives
    % \[\Prob\left( \widebar{x}-z_{1-\alpha/2}\frac{s}{\sqrt{n}}<\mu<\widebar{x}+z_{1-\alpha/2}\frac{s}{\sqrt{n}}\right)=1-\alpha.\] 
    % As such, a \((1-\alpha)100\%\) confidence interval is
    % \[\left( \widebar{x}-z_{1-\alpha/2}\frac{s}{\sqrt{n}}\,,\ \widebar{x}+z_{1-\alpha/2}\frac{s}{\sqrt{n}} \right).\]
    % When the variance \(\sigma^2\) is known, we can replace \(s\) with \(\sigma\).
    \item Let \(\hat{\theta}\) be a statistic that is normally distributed with mean \(\theta\) and standard error \(\sigma_{\hat{\theta}}\). We see that 
    \[\frac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}}=Z \sim \Normal(0,1).\]
    Rewriting \(\Prob(-z_{1-\alpha/2}<Z<z_{1-\alpha/2})=1-\alpha\) gives
    \[\Prob(\hat{\theta}-z_{1-\alpha/2}\sigma_{\hat{\theta}}<\theta<\hat{\theta}+z_{1-\alpha/2}\sigma_{\hat{\theta}})=1-\alpha.\]
    Hence, a \((1-\alpha)100\%\) confidence interval for \(\theta\) is
    \[(\hat{\theta}-z_{1-\alpha/2}\sigma_{\hat{\theta}}\,,\ \hat{\theta}+z_{1-\alpha/2}\sigma_{\hat{\theta}}).\]
    \href{https://www.amazon.sg/Mathematical-Statistics-Applications-William-Mendenhall/dp/0495110817#customerReviews}{(Wackerly-Mendenhall-Schaefer)}
    \item Let \(0<\alpha<1\) and \(X_1,X_2,\dots,X_n\) be a sample on a random variable \(X\) with mean \(\mu\), where \(n\) is large. Then, an approximate \((1-\alpha)100\%\) confidence interval for \(\mu\) is
    \[\left( \widebar{x}-z_{1-\alpha/2}\frac{s}{\sqrt{n}}\,,\ \widebar{x}+z_{1-\alpha/2}\frac{s}{\sqrt{n}} \right).\]
    When the variance \(\sigma^2\) is known, we can replace \(s\) with \(\sigma\). If the distribution of \(X\) is known to be normal, in addition to \(\sigma^2\) being known exactly, then the confidence interval is exact; it is not just an approximation. 

    \href{https://www.amazon.com/Introduction-Mathematical-Statistics-8th-Whats-dp-0134686993/dp/0134686993/ref=dp_ob_title_bk}{(Hogg-McKean-Craig)}
    \item Let \(X\) be a Bernoulli random variable with probability of success \(p\), where \(X\) is 1 or 0 if the outcome is success or failure, respectively. Suppose \(X_1,X_2,\dots,X_n\) is a random sample from the distribution of \(X\), where \(n\) is large. Let \(\hat{p}=\widebar{X}\) be the sample proportion of successes. Then, an approximate \((1-\alpha)100\%\) confidence interval for \(p\) is given by 
    \[\left( \hat{p}-z_{1-\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\,,\ \hat{p}+z_{1-\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \right).\]
    (Letting \(Y=X_1+X_2+\dots+X_n\sim \operatorname{B}(n,p)\) gives \(\hat{p}=Y/n\), which is the presentation used in the school's notes.) 

    \href{https://www.amazon.com/Introduction-Mathematical-Statistics-8th-Whats-dp-0134686993/dp/0134686993/ref=dp_ob_title_bk}{(Hogg-McKean-Craig)}
  \end{itemize}
\end{stbox}
\begin{note}
  Standard phrasing for the interpretation of a \((1-\alpha)100\%\) confidence interval \((a,b)\). 
  \begin{center}
    The probability that the interval \((a,b)\) contains the true value of the [population mean/proportion in context] is \(1-\alpha\).
  \end{center}
\end{note}
\begin{note}
  Standard phrasing for what is a \((1-\alpha)100\%\) confidence interval for \(\theta\)?
  \begin{center}
    It is an interval which has probability \(1-\alpha\) of containing the true value of \(\theta\).  
  \end{center}
\end{note}
\begin{note}
  Standard phrasing for whether mean/proportion in context has likely increased/decreased, when given suitable confidence intervals.
  \begin{enumerate}
    \item There is no conclusive result.

    Since the old and new \((1-\alpha)\%\) confidence intervals overlap, we are unable to conclude whether the [mean/proportion in context] has decreased or not. Hence, it is inconclusive from these figures as to whether the [context (e.g. an awareness campaign)] has been effective.
    \item It has likely increased/decreased.
    
    The old \((1-\alpha)\%\) confidence interval is to the left/right of the new \((1-\alpha)\%\) confidence interval, such that they do not overlap. So, can conclude that the [mean/proportion in context] likely increased/decreased. Hence, these figures suggests that the [context (e.g. an awareness campaign)] has been effective.
  \end{enumerate}
\end{note}
\begin{note}
  Advantage and disadvantage of a \((1-\beta)\%\) confidence interval compared to a \((1-\alpha)\%\) confidence interval, where \(\beta<\alpha\).
  \begin{center}
    \begin{tabular}{ll}
      Advantage:& A \((1-\beta)\%\) CI is more likely to contain the true mean.\\
      Disadvantage:& A \((1-\beta)\%\) CI is less precise (or wider).
    \end{tabular}
  \end{center}
  \emph{Note.} Clearly state which is the advantage and disadvantage, as illustrated above.
\end{note}
\begin{GCSkills}{}
  Calculating statistics (i.e. \(\widebar{x}\), \(s\), etc) by G.C. given data for a sample.
  \begin{enumerate}
    \item Keying in the data: \texttt{stat} \(\Longrightarrow\) \texttt{1:Edit} \(\Longrightarrow\) Key in the data into one of the lists \(\texttt{L}_i\). 
    \item Calculating the statistic: \texttt{stat} \(\Longrightarrow\) \texttt{CALC} \(\Longrightarrow\) \texttt{1-Var Stats} (\texttt{List:}\(\texttt{L}_i\)) \(\Longrightarrow\) \texttt{Calculate}.
    \item Getting the statistic for further calculations: \texttt{vars} \(\Longrightarrow\) \texttt{5:Statistics} \(\Longrightarrow\) Select the desired statistic.
  \end{enumerate}
\end{GCSkills}
\begin{GCSkills}{}
  Calculating the symmetric confidence interval for a normally distributed random variable.
  \begin{center}
  \begin{tabular}{ll}
    Mean: & \texttt{stat} \(\Longrightarrow\) \texttt{TESTS} \(\Longrightarrow\) \texttt{7:ZInterval\dots}\\
    Proportion: & \texttt{stat} \(\Longrightarrow\)\texttt{TESTS} \(\Longrightarrow\) \texttt{A:1-PropZInt\dots}\\ 
  \end{tabular}
  \end{center}
\end{GCSkills}

\chapter{Statistics: Hypothesis Testing}
\section{General Information}
\begin{definition}{}{}
  The \emph{null hypothesis} \(H_0\) and \emph{alternative hypothesis} \(H_1\) are the hypotheses that we hope to reject and accept, respectively. 
\end{definition}
\begin{stbox}{General Information}
  \begin{itemize}
    \item Without going into details, a \emph{critical region} \(C\) is just a set that defines the decision rule / test 
    \begin{center}
      Reject \(H_0\) (Accept \(H_1\)) \quad if \((X_1,X_2,\dots,X_n)\in C\),
    \end{center}
    for any random sample \(X_1,X_2,\dots,X_n\) from the distribution of a random variable \(X\).
  \end{itemize}
\end{stbox}
\begin{definition}{}{}
  The \emph{significance level} \(100\alpha\%\) of a test is the probability of rejecting \(H_0\) when it is in fact true. i.e. \(\alpha=\Prob(H_0\text{ is rejected}\,\vert\,H_0\text{ is true})\).
\end{definition}
\begin{note}
  Explain, in context, the meaning of `at the \(\alpha\%\) level of significance'.
  \begin{center}
    \parbox{0.9\textwidth}{
      The probability that [\(H_1\) in context], when actually [\(H_0\) in context], is \(\alpha\%\).
    }
  \end{center}
\end{note}
\begin{definition}{}{}
  The \emph{\(p\)-value} is the lowest level of significance for which the null hypothesis will be rejected. In other words, for the null hypotheses
    \begin{center}
      \begin{enumerate*}[label=(\alph*),itemjoin={\quad}]
        \item \(\mu<\mu_0\),
        \item \(\mu \neq \mu_0\),
        \item \(\mu>\mu_0\),
      \end{enumerate*}
    \end{center}
    we have
    \begin{center}
      \begin{enumerate*}[label=(\alph*),itemjoin={\quad}]
        \item \(p\text{-value}=\Prob(Z\leq z_\text{calc})\),
        \item \(p\text{-value}=\Prob(\lvert Z \rvert\geq \lvert z_\text{calc} \rvert)\),
        \item \(p\text{-value}=\Prob(Z\geq z_\text{calc})\).
      \end{enumerate*}
    \end{center}
\end{definition}
\begin{note}
  Explain what the \(p\)-value means in context.
  \begin{center}
    \parbox{0.9\textwidth}{
      The \(p\)-value is the least level of significance to conclude that [\(H_1\) in context].
    }
  \end{center}
\end{note}
\begin{stbox}{}
  \begin{itemize}
    \item One sample \(z\)-test. There are various combination of assumptions for which this test applies. For brevity, we shall avoid restating it, instead directing the reader to table \ref{Table:Summary table for one-sample hypothesis testing.}
    \begin{enumerate}
      \item Let [\(X\) in context] and \(\mu\) be the population mean.
      \item 
      \begin{tabular}{|ll|}
        \hline
        Test & \(H_0\colon\mu=\mu_0\)\\
        against &\(H_1\colon\) 
        \begin{enumerate*}[itemjoin={\quad}]
          \item \(\mu<\mu_0\),
          \item \(\mu \neq \mu_0\),\quad or
          \item \(\mu>\mu_0\),
        \end{enumerate*}\\
        \multicolumn{2}{|l|}{at the \(100\alpha\%\) significance level.}\\
        \hline
      \end{tabular}
      \item Under \(H_0\), we have \(\widebar{X}\sim \Normal(\mu_0,\hat{\sigma}^2/n)\) approximately. Or, if \(\sigma^2\) is known exactly, then by CLT \(\widebar{X}\sim \Normal(\mu_0,\sigma^2/n)\) approximately.
      \item Test statistic: 
      \[Z=\frac{\widebar{X}-\mu_0}{\sigma/\sqrt{n}}\sim\Normal(0,1).\]
    \end{enumerate}
      \begin{minipage}[t]{0.45\textwidth}
        \begin{enumerate}
          \setcounter{enumi}{3}
          \item Find \(z_{1-\alpha}\) or \(z_{1-\alpha/2}\), which satisfies
          \begin{enumerate}
            \item \(\Prob(Z<z_{1-\alpha})=\alpha\), 
            \item \(\Prob(-z_{1-\alpha/2}<Z<z_{1-\alpha/2})=1-\alpha\), or
            \item \(\Prob(Z>z_{1-\alpha})\).
          \end{enumerate}
          \item Find the test statistic value 
          \[z_\text{calc}=\frac{\hat{\mu}-\mu_0}{\sigma/\sqrt{n}}.\]
          \item Reject \(H_0\) iff 
          \begin{enumerate}
            \item \(z_\text{calc}<z_{1-\alpha}\),
            \item \(\lvert z_\text{calc} \rvert>z_{1-\alpha/2}\), or
            \item \(z_\text{calc}>z_{1-\alpha}\).
          \end{enumerate}
        \end{enumerate}
      \end{minipage}
      \begin{minipage}[t]{0.45\textwidth}
        \begin{enumerate}
          \setcounter{enumi}{3}
          \item Find the \(p\)-value using GC.
          \item Reject \(H_0\) iff \(p\)-value is less than \(\alpha\).
        \end{enumerate}
        \vfill
        \begin{flushright}
          \begin{minipage}{0.8\textwidth}
            \begin{GCSkills}{}
              Calculating the \(p\)-value of a sample.
              \begin{center}
                \texttt{stat \(\Longrightarrow\) TESTS \(\Longrightarrow\) 1:Z-Test\dots}
              \end{center}
            \end{GCSkills}
          \end{minipage}
        \end{flushright}
      \end{minipage}
      \begin{enumerate}
        \item[7.] Since\quad
        \begin{enumerate*}[label=(\alph*),itemjoin={\quad}]
          \item \(z_\text{calc}<z_{1-\alpha}\),
          \item \(\lvert z_\text{calc} \rvert>z_{1-\alpha/2}\),
          \item \(z_\text{calc}>z_{1-\alpha}\),\quad or\quad \(p\text{-value}<\alpha\),
        \end{enumerate*}
        we reject \(H_0\). There is sufficient evidence at the significance level \(100\alpha\%\) that [\(H_1\) in context].

        \emph{Note.} For \emph{not} rejecting \(H_0\), simply change to the appropriate inequality (such that \(z_\text{calc}\) is outside the critical region) and write ``insufficient'' instead of ``sufficient''.
      \end{enumerate}
      \item If we have a null hypothesis, such as
      \begin{center}
        \(H_0\colon\mu\leq\mu_0\)\quad or\quad \(H_0\colon\mu\geq\mu_0\),
      \end{center}
      we can just use \(H_0\colon\mu=\mu_0\) instead.
  \end{itemize}
\end{stbox}
% \begin{definition}{}{}
%   We say a critical region \(C\) is of \emph{size} \(\alpha\) if 
%   \[\alpha=\max_{\theta\in\omega_0}P_\theta[(X_1,X_2,\dots,X_n)\in C].\]
%   In our syllabus, \(\alpha\cdot100\%\) is called the \emph{significance level}.
% \end{definition}
\begin{note}
  Explain why there is no need to assume that the distribution of \(X\) is normal/know anything about the population distribution of \(X\).
  \begin{center}
    \parbox{0.9\textwidth}{
      As the \hly{sample size \(n\) is large}, by the \hly{Central Limit Theorem}, the \hly{sample mean} of [random variable \(X\) in context] will \hly{approximately follow a normal distribution}.
    }
  \end{center}
  \emph{Note.} Spell ``Central Limit Theorem'' and ``the sample mean'' out \emph{in full}. Do not use CLT or \(\widebar{X}\) for this question.
\end{note}
\begin{definition}{}
  A random variable \(X\) follows Student's \(t\)-distribution with \(\nu\) degrees of freedom iff its pdf is
  \[f(t)=\frac{\Gamma{\left( \frac{\nu+1}{2} \right)}}{\sqrt{\pi\nu}\ \Gamma{\left( \frac{\nu}{2} \right)}}\left( 1+\frac{t^2}{\nu} \right)^{-\frac{1}{2}(\nu-1)}.\]
  This is denoted by \(X\sim t(\nu)\).
\end{definition}
\begin{stbox}{}
  \begin{itemize}
    \item Properties of Student's \(t\)-distribution.
    \begin{enumerate}
      \item It is continuous and symmetric about the vertical axis, i.e. \(t=0\).
      \item As \(\nu\to\infty\), we have \(t(\nu)\to\Normal(0,1)\).
    \end{enumerate}
    \item Let \(T\sim t(n-1)\) and \(t_{(n-1,1-\alpha/2)}\) be such that \(\Prob{\left(-t_{(n-1,1-\alpha/2)}<T<t_{(n-1,1-\alpha/2)}\right)}=1-\alpha\). A \((1-\alpha)100\%\) confidence interval, for the population mean \(\mu\) of \(T\), is
    \[\left( \widebar{x}-t_{(n-1,1-\alpha/2)}\frac{s}{\sqrt{n}}\,,\ \widebar{x}-t_{(n-1,1-\alpha/2)}\frac{s}{\sqrt{n}} \right).\]
    \item Suppose we are conducting the following test:
    \begin{center}
      \begin{tabular}{|ll|}
        \hline
        Test & \(H_0\colon\mu=\mu_0\)\\
        against &\(H_1\colon\mu\neq\mu_0\)\\
        \multicolumn{2}{|l|}{at a \(100\alpha\%\) significance level.}\\
        \hline
      \end{tabular}
    \end{center}
    Then, we reject \(H_0\) iff the appropriate symmetric interval (\(z\) or \(t\)-interval) does \emph{not} contain \(\mu_0\). 
  \end{itemize}
\end{stbox}
\begin{GCSkills}{}
  Calculating the symmetric \(t\)-confidence interval, for the population mean, of a random variable following Student's \(t\)-distribution.
  \begin{center}
    \texttt{stat} \(\Longrightarrow\) \texttt{TESTS} \(\Longrightarrow\) \texttt{8:TInterval\dots} 
  \end{center}
\end{GCSkills}
\begin{stbox}{}
  \begin{itemize}
    \item A one sample \(t\)-test.
    % We use this test when \emph{the population variance is unknown} and the sample size is small.
    Again, see table \ref{Table:Summary table for one-sample hypothesis testing.} for the necessary assumptions.
    \begin{enumerate}
      \item Let [\(X\) in context], \emph{which we assume to be normally distributed}, and \(\mu\) be the population mean.
      \item 
      \begin{tabular}{|ll|}
        \hline
        Test & \(H_0\colon\mu=\mu_0\)\\
        against &\(H_1\colon\) 
        \begin{enumerate*}[itemjoin={\quad}]
          \item \(\mu<\mu_0\),
          \item \(\mu \neq \mu_0\),\quad or
          \item \(\mu>\mu_0\),
        \end{enumerate*}\\
        \multicolumn{2}{|l|}{at the \(100\alpha\%\) significance level.}\\
        \hline
      \end{tabular}
      \item Under \(H_0\), the test statistic
      \[T=\frac{\widebar{X}-\mu}{s/\sqrt{n}}\sim t(n-1).\]
      \item Continue as per usual, calculating the critical region or the \(p\)-value.
    \end{enumerate}
  \end{itemize}
\end{stbox}
\begin{GCSkills}{}
  Calculating, for a one sample \(t\)-test, the 
  \begin{center}
    \begin{tabular}{ll}
      \(p\)-value: & \texttt{stat} \(\Longrightarrow\) \texttt{TESTS} \(\Longrightarrow\) \texttt{2:T-Test\dots}\\
      critical region: & \texttt{2nd} \(\Longrightarrow\) \texttt{vars} \(\Longrightarrow\) \texttt{4:invT(}
    \end{tabular}
  \end{center}
\end{GCSkills}
\begin{note}
  In the GC, \texttt{invT} is always `to the \texttt{LEFT}'. That is, the output \(t\) of 
  \begin{center}
    \begin{tabular}{|lScr|}
      \hline
      &\qquad\colorbox{black}{\textcolor{white}{\texttt{invT}}}&\\
      \texttt{area:\(A\)}&&\qquad\hphantom{\texttt{area:\(A\)}}\\
      \texttt{df:\(\nu\)}&&\\
      \texttt{Paste}&&\\
      \hline
    \end{tabular}
  \end{center}
  is such that \(\Prob(T<t)=A\).
\end{note}
\begin{stbox}{}
  \begin{itemize}
    \item A two-sample \(z\)-test. 
    % Let \(X_1\) and \(X_2\) have population means \(\mu_1\) and \(\mu_2\); population variances \(\sigma_1^2\) and \(\sigma_2^2\) (respectively). Further suppose we have two independent random samples, from the distributions of \(X_1\) and \(X_2\), each of sizes \(n_1\) and \(n_2\) (respectively). If (i) or (ii) is true, then we carry out a two-sample \(z\)-test.
    Again, see table \ref{Table:Summary table for two-sample hypothesis testing.} for the necessary assumptions.
    \begin{enumerate}[label=(\roman*)]
      \item \(\sigma_1\) and \(\sigma_2\) are known, in addition to
      \begin{enumerate}[label=(\arabic*)]
        \item \(X_1\) and \(X_2\) being normally distributed, or
        \item both sample sizes, \(n_1\) and \(n_2\), being large.
      \end{enumerate} 
      \item \(\sigma_1\) and \(\sigma_2\) are unknown, but \(X_1\) and \(X_2\) are normally distributed, and both samples are large (so we can use the fact that a \(t\)-distribution approximates to a normal distribution with large sample sizes).
    \end{enumerate}
    \begin{enumerate}
      \item Let [\(X_1\), \(X_2\) in context], (which we assume to be normally distributed)\footnote{if applicable} and \(\mu\) be the population mean.
      \item 
      \begin{tabular}{|ll|}
        \hline
        Test & \(H_0\colon\mu_1-\mu_2=c\)\\
        against &\(H_1\colon\)
        \begin{enumerate*}[itemjoin={\quad}]
          \item \(\mu_1-\mu_2<c\),
          \item \(\mu_1-\mu_2=c\),\quad or
          \item \(\mu_1-\mu_2>c\),
        \end{enumerate*}\\
        \multicolumn{2}{|l|}{at the \(100\alpha\%\) significance level.}\\
        \hline
      \end{tabular}
      \item Under \(H_0\), the test statistic
      \begin{enumerate}[align=parleft,label=(\roman*)]
        \item \[Z=\frac{(\widebar{X}_1-\widebar{X}_2)-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\sim\Normal(0,1).\]
        \item[(ii)(1)]\[Z=\frac{(\widebar{X}_1-\widebar{X}_2)-(\mu_1-\mu_2)}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}\sim\Normal(0,1).\]
        \item[(ii)(2)] \[Z=\frac{(\widebar{X}_1-\widebar{X}_2)-(\mu_1-\mu_2)}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim\Normal(0,1)\quad\text{where}\quad s_p^2=\rule{0.5cm}{0.1mm}.\]
      \end{enumerate}
      Case (ii)(2) is used when the population variances coincide, i.e. \(\sigma_1=\sigma_2\).
      \item Continue as per usual, calculating the critical region or the \(p\)-value.
    \end{enumerate}
  \end{itemize}
\end{stbox}
\begin{recall}
  \[s_p^2=\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}.\]
\end{recall}
\begin{stbox}{}
  \begin{itemize}
    \item A two-sample \(t\)-test. 
    % Let \(X_1\sim\Normal(\mu_1,\highlight[yellow]{\sigma^2})\) and \(X_2\sim\Normal(\mu_2,\highlight[yellow]{\sigma^2})\). Further suppose we have two independent random samples, from the distributions of \(X_1\) and \(X_2\); of small sizes \(n_1\) and \(n_2\) (respectively). Then we carry out a two-sample \(t\)-test.
    Again, see table \ref{Table:Summary table for two-sample hypothesis testing.} for the necessary assumptions.
    \begin{enumerate}
      \item Let [\(X_1\), \(X_2\) in context], \emph{which we assume to be normally distributed}, and \(\mu\) be the population mean.
      \item 
      \begin{tabular}{|ll|}
        \hline
        Test & \(H_0\colon\mu_1-\mu_2=c\)\\
        against &\(H_1\colon\)
        \begin{enumerate*}[itemjoin={\quad}]
          \item \(\mu_1-\mu_2<c\),
          \item \(\mu_1-\mu_2=c\),\quad or
          \item \(\mu_1-\mu_2>c\),
        \end{enumerate*}\\
        \multicolumn{2}{|l|}{at the \(100\alpha\%\) significance level.}\\
        \hline
      \end{tabular}
      \item Under \(H_0\), the test statistic
      \[T=\frac{(\widebar{X}_1-\widebar{X}_2)-(\mu_1-\mu_2)}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim t(n_1+n_2-2)\quad\text{where}\quad s_p^2=\rule{0.5cm}{0.1mm}.\]
      \item Continue as per usual, calculating the critical region or the \(p\)-value.
    \end{enumerate}
  \end{itemize}
\end{stbox}
\begin{GCSkills}{}
  Calculating the \(p\)-value for a
  \begin{center} 
    \begin{tabular}{ll}
      two-sample \(z\)-test: & \texttt{stat} \(\Longrightarrow\) \texttt{TESTS} \(\Longrightarrow\) \texttt{3:2-SampZTest\dots}\\
      two-sample \(t\)-test: & \texttt{stat} \(\Longrightarrow\) \texttt{TESTS} \(\Longrightarrow\) \texttt{4:2-SampTTest\dots} \(\Longrightarrow\) \hly{\texttt{Pooled:Yes}}
    \end{tabular}
  \end{center}
\end{GCSkills}
\begin{stbox}{}
  \begin{itemize}
    \item A paired sample \(t\)-test. 
    % Let \(X\) and \(Y\)be normally distributed, with population means \(\mu_1\) and \(\mu_2\); population variances \(\sigma_1^2\) and \(\sigma_2^2\). (respectively). We define \(D\coloneq X-Y\). Further suppose we have two independent random samples \(X_1,X_2,\dots,X_n\) and \(Y_1,Y_2,\dots,Y_n\), such that 
    % \begin{enumerate}[label=(\roman*)]
    %   \item \(D_1,D_2,\dots,D_n\) are normally distributed
    %   \item the data within each pair \(\{X_i,Y_i\}\) are dependent on each other, but
    %   \item pairs \(\{X_i,Y_i\}\) and \(\{X_j,Y_j\}\) are independent of each other, for \(i\neq j\).
    % \end{enumerate}
    % Then, we use a paired sample \(t\)-test. 
    Again, see table \ref{Table:Summary table for two-sample hypothesis testing.} for the necessary assumptions.
  \end{itemize}
  \begin{enumerate}
    \item Let \(D=\text{[\(X\) in context]}-\text{[\(Y\) in context]}\), and \(\mu_D\) be the population mean.
    \item 
    \begin{tabular}{|ll|}
      \hline
      Test & \(H_0\colon\mu_D=\mu_0\)\\
      against &\(H_1\colon\) 
      \begin{enumerate*}[itemjoin={\quad}]
        \item \(\mu_D<\mu_0\),
        \item \(\mu_D \neq \mu_0\),\quad or
        \item \(\mu_D>\mu_0\),
      \end{enumerate*}\\
      \multicolumn{2}{|l|}{at the \(100\alpha\%\) significance level.}\\
      \hline
    \end{tabular}
    \item Under \(H_0\), the test statistic
    \[T=\frac{\widebar{D}-\mu_0}{s_D/\sqrt{n}}\sim t(n-1).\]
    \item \(d=x_1-y_1,x_2-y_2,\dots,x_n-y_n\) (insert contextual values) so
    \[\widebar{d}=\rule{0.5cm}{0.1mm}\qquad\text{and}\qquad s_d^2=\frac{1}{n-1}\left( \sum{d^2}-\frac{\left( \sum{d} \right)^2}{n} \right)=\rule{0.5cm}{0.1mm}.\]
    \item Continue as per usual, calculating the critical region or the \(p\)-value.
  \end{enumerate}
\end{stbox}

\begin{landscape}
  \section{Summary}
Throughout the following table, we \emph{always} assume that the (both) sample(s) independent and random.
  \begin{table}[htbp]
      \begin{tabular}{|Sc|Sc|}
        \hline
        Assumptions/Reasons & Test (Statistic)\\
        \hline
        \begin{minipage}{418.6pt}
          \begin{enumerate}[align=parleft]
            \item[{[ii]}]\ The variance \(\sigma^2\) is known.
            \item[{[ii]}(1)]\ Sample size \(n\) is large (so CLT applies).
            \item[{[ii]}(2)]\ Sample size \(n\) is small, but we assume \(X\) is normally distributed.
          \end{enumerate}
        \end{minipage}&
        \begin{minipage}{179.4pt}
          \begin{center}
            One-sample \(z\)-test
            \[Z=\frac{\widebar{X}-\mu_0}{\sigma/\sqrt{n}}\sim\Normal(0,1)\]
            (approximately if CLT was used)
          \end{center}
        \end{minipage}\\
        \hline
        \begin{minipage}{418.6pt}
          \begin{enumerate}[align=parleft]
            \item[{[i]}]\ The variance \(\sigma^2\) is unknown.
            \item[{[ii]}]\ Sample size \(n\) is large.
            \item[{[iii]}(1)]\ \(X\) is known to be normally distributed.
          \end{enumerate}
          \begin{enumerate}[leftmargin=3cm,labelindent=-\leftmargin,align=parleft,labelwidth=\widthof{(H2 Math)}]
            \item[(FM)] So \(t(n-1)\) approximates to \(\Normal(0,1)\).
            \item[(H2 Math)] No specific reason, just write ``approximately.''.  
          \end{enumerate}
          \begin{enumerate}[align=parleft]
            \item[{[iii]}(2)]\ \(X\) is not known to be normally distributed.        
          \end{enumerate}
          \begin{enumerate}[leftmargin=5cm,labelindent=-\leftmargin,align=parleft,labelwidth=\widthof{(H2 Math Handwaving)}]
            \item[(H2 Math Handwaving)] CLT applies.  
          \end{enumerate}
        \end{minipage}&
        \begin{minipage}{179.4pt}
          \begin{center}
            One-sample \(z\)-test
            \[Z=\frac{\widebar{X}-\mu_0}{s/\sqrt{n}}\sim\Normal(0,1)\]
            (approximately)
          \end{center}
        \end{minipage}\\
        \hline
        \begin{minipage}{418.6pt}
          \begin{enumerate}[label={[\roman*]},align=parleft]
            \item The variance \(\sigma^2\) is unknown.
            \item Sample size \(n\) is small.
            \item Assume \(X\) is normally distributed.
          \end{enumerate}
        \end{minipage}&
        \begin{minipage}{179.4pt}
          \begin{center}
            One-sample \(t\)-test
            \[T=\frac{\widebar{X}-\mu_0}{s/\sqrt{n}}\sim t(n-1)\]
          \end{center}
        \end{minipage}\\
        \hline
      \end{tabular}
    \caption{Summary table for one-sample hypothesis testing.}
    \label{Table:Summary table for one-sample hypothesis testing.}
  \end{table}
\end{landscape}
\begin{landscape}
  \begin{table}[htbp]
    \begin{tabular}{|Sc|Sc|}
      \hline
      Assumptions/Reasons & Test (Statistic)\\
      \hline
      \begin{minipage}{418.6pt}
        \begin{enumerate}[align=parleft]
          \item[{[i]}]\ Both variances \(\sigma_1\) and \(\sigma_2\) are known.
          \item[{[ii](1)}]\ Both sample sizes \(n_1\) and \(n_2\) are large (so CLT applies).
          \item[{[ii](2)}]\ Either sample size \(n_1\) or \(n_2\) is small, but we assume \(X_1\) and \(X_2\) are normally distributed.
        \end{enumerate}
      \end{minipage}&
      \begin{minipage}{179.4pt}
        \begin{center}
          Two-sample \(z\)-test
          \[Z=\frac{\widebar{X}_1-\widebar{X}_2-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\sim\Normal(0,1)\]
          (approximately if CLT was used)
        \end{center}
      \end{minipage}\\
      \hline
      \begin{minipage}{418.6pt}
        \begin{enumerate}[label={[\roman*]},align=parleft]
          \item One of the variances \(\sigma_1\) and \(\sigma_2\) are unknown.
          \item Both sample sizes \(n_1\) and \(n_2\) are large.
          \item Assume \(X_1\) and \(X_2\) are normally distributed.
        \end{enumerate}
        So \(t(n_1+n_2-2)\) approximates to \(\Normal(0,1)\).
      \end{minipage}&
      \begin{minipage}{179.4pt}
        \begin{center}
          Two-sample \(z\)-test
          \[Z=\frac{\widebar{X}_1-\widebar{X}_2-(\mu_1-\mu_2)}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}\sim\Normal(0,1)\]
          approximately
        \end{center}
      \end{minipage}\\
      \hline
      \begin{minipage}{418.6pt}
        \begin{enumerate}[label={[\roman*]},align=parleft]
          \item Both variances \(\sigma_1^2\) and \(\sigma_2^2\) coincide.
          \item Assume \(X_1\) and \(X_2\) are normally distributed. (Alt: Both samples come from normal populations.)
        \end{enumerate}
      \end{minipage}&
      \begin{minipage}{179.4pt}
        \begin{center}
          Two-sample \(t\)-test
          \[T=\frac{\widebar{X}_1-\widebar{X}_2-(\mu_1-\mu_2)}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim t(n_1+n_2-2)\]
        \end{center}
      \end{minipage}\\
      \hline
      \begin{minipage}{418.6pt}
        \begin{enumerate}[align=parleft]
          \item Assume that \(D_1,D_2,\dots,D_n\) are normally distributed.
          \item Assume that the data within each pair \(\{X_i,Y_i\}\) are dependent on each other, but pairs \(\{X_i,Y_i\}\) and \(\{X_j,Y_j\}\) are independent of each other, for \(i\neq j\). 
        \end{enumerate}
      \end{minipage}&
      \begin{minipage}{179.4pt}
        \begin{center}
          Paired-sample \(t\)-test
        \[T=\frac{\widebar{D}-\mu_D}{s_D/\sqrt{n}}\sim t(n-1).\]
        \end{center}
      \end{minipage}\\
      \hline
    \end{tabular}
  \caption{Summary table for two-sample hypothesis testing.}
  \label{Table:Summary table for two-sample hypothesis testing.}
\end{table}
\end{landscape}

\chapter{Correlation and Linear Regression}
\begin{note}
  A good scatter diagram should follow the guidelines below.
  \begin{itemize}
    \item The relative position of each point on the scatter diagram should be clearly shown.
    \item The range of values for the set of data should be clearly shown by marking out the extreme \(x\) and \(y\) values on the corresponding axis.
    \item The axes should be labeled clearly with the variables.
  \end{itemize}
\end{note}
\begin{stbox}{General Information}
  \begin{itemize}
    \item The Product Moment Correlation Coefficient is a measure of the linear correlation between two variables. It is defined by
    \[r=\frac{\sum{(x-\bar{x})(y-\bar{y})}}{\sqrt{\sum{(x-\bar{x})^2}\sum{(y-\bar{y})^2}}}=\frac{\sum{xy}-\dfrac{\sum{x}\sum{y}}{n}}{\sqrt{\left[\sum{x^2}-\dfrac{\left(\sum{x}\right)^2}{n}\right]\left[\sum{y^2}-\dfrac{\left(\sum{y}\right)^2}{n}\right]}},\]
    which takes on a value from 0 to 1.
    \item When \(r=0\), there is no linear relationship. But, a nonlinear relationship may be present. Additionally, the regression lines are perpendicular.
    \item The closer the value of \(r\) is to 1 (or -1), the stronger the positive (or negative) linear correlation. Furthermore, the regression lines coincide.
    \begin{center}
      \includegraphics[scale=0.3]{../images/Product Moment Correlation Coefficient 1.png}
      \includegraphics[scale=0.4]{../images/Product Moment Correlation Coefficient 2.png}
    \end{center}
    \item The regression line of \(y\) on \(x\) minimises the sum of squares deviation (error) in the \(y\)-direction. (i.e. we are assuming \(x\) is the independent variable whose values are known exactly.) It is given by
    \[y=\bar{y}+b(x-\bar{x}),\qquad\text{where}\qquad b=\frac{\sum{(x-\bar{x})(y-\bar{y})}}{\sum{(x-\bar{x})^2}}=\frac{\sum{xy}-\dfrac{\sum{x}\sum{y}}{n}}{\sum{x^2}-\dfrac{\left(\sum{x}\right)^2}{n}}.\] 
    \item The point \((\bar{x},\bar{y})\) always lies on both the regression lines of \(y\) on \(x\), and \(x\) on \(y\).
    \item Say we are given the value of one variable, and asked to approximate the the value of the other variable. Then, we should always use the line of the \emph{dependent} variable on the \emph{independent}.
    \item Estimations should not be taken for data outside the range of the sample provided, even if the value of \(r\) is close to 1.
  \end{itemize}
\end{stbox}
\end{document}